---
title: "[번역]StarCraft2 A New Challenge for Reinforcement Learning"
categories:
  - PaperTranslation
tags:
  - ReinforcementLearning
mathjax: True
---

# 스타크래프트2 AI 환경설정
원문 : StarCraft2 A New Challenge for Reinforcement Learning

<br />

## 초록
이 논문은 스타크래프트2를 기반으로 하는 강화 학습 환경을 소개한다.
이는 여러 면에서 기존 강화학습에 비해 새롭고 광활한 뿐 아니라, 더욱 도전적이다. 

* 다수의 유저가 상호작용하는 멀티-에이전트 문제
* 일부만 관찰된 맵에 의한 불완전한 정보
* 수백 개의 유닛에 대한 선택과 컨트롤을 수반하는 광범위한 행동공간(action space) 
* raw input 피쳐 평면으로 인한 넓은 상태공간(state space)
* 수천번의 단계 동안 장기전략(long-term strategies)이 요구되는 신뢰할당(credit assignment) 딜레이 문제.

우리는 스타크래프트2에 대해 관측(observation), 행동(action), 보상(reward)을 설명하고, 
게임엔진과 의사소통하는 파이썬 베이스 인터페이스를 오픈소스로 제공한다.
메인 게임 맵 뿐 만 아니라, 우리는 스타크래프트2의 서로 다른 요소에 집중하는 미니 게임을 제공한다.
메인 게임 맵에 대해선, 우리는 실제 고수 유저의 리플레이 데이터셋을 제공한고,
이 데이터를 이용해 신경망을 이용하여 학습시킨 초기 베이스라인이 되는 결과 또한 제공한다.
이는 게임 결과와 유저의 행동 예측에 사용된다. 
즉, 스타크래프트2에 적용되는 표준(canonical) 심층강화학습 에이전트로 사용된다는 뜻이다.
미니게임에서의 플레이 목표 수준은 초보 유저와 비교할수 있을 정도이다.
그러므로, 스타크래프트2 강화 학습 환경은 심층강화학습 알고리즘과 아키텍쳐에 대해 새롭운 도전을 제공한다.
<br />
 
## 1.인트로
최근 발전하고 있는 음성인식, 컴퓨터 비전, 자연어처리는 신경망을 이용해 비선형 점근 함수를 구하기 위한 
강력한 툴을 제공함으로써 딥러닝의 부활에 기여했다. 또한 이러한 기술은 아타리 게임, 바둑, 3차원 가상 환경과 로보틱스 분야에서
유의미한 성공을 보이는 강화학습에서도 성공적이라는 것이 입증되었다. 이러한 큰 성공은 어려운 난이도의 분야에서도 시뮬레이션 되었다.
이러한 벤치마크는 진보된 딥러닝과 강화학습 연구결과에 중요한 기여를 하였다. 
이는 1차원 이상에서 현재 방법의 역량을 뛰어넘는 도메인의 유효성을 확실히 하는 면에서 대단히 중요하다.
본 논문에서는 새롭고 도전적인 강화학습 도메인인 스타크래프트2라는 게임의 SC2LE(스타크래프트2 학습환경)을 소개한다.
스타크래프트는 실시간 전략 시뮬레이션(RTS) 게임인데, 빠른 micro-action과 높은 수준의 전략수립과 실행능력을 요구한다. 
지난 20년 동안, 스타크래프트1과 2는 e-sports의 개척자였으며, 수많은 스타 플레이어를 탄생시켰다.
따라서 최고 수준의 플레이어를 이기는 것은 의미있으며 장기적인 목표가 될 수 있다.
<br />
강화학습 관점으로 스타크래프트2는 기존의 한계를 뛰어넘을 만한 최고의 기회를 제공한다. 
첫째, 스타크래프트2는 여러 명의 플레이어가 자원을 놓고 경쟁하는 멀티에이전트 문제이다.
또한 각 플레이어들은 공통 목표를 달성하기 위해 수백개의 유닛을 컨트롤 하며 협동하는 lower-level 멀티에이전트라고 볼 수 있다. 
둘째, 스타크래프트2는 불완전한 정보 게임이다. 맵은 오직 로컬 카메라를 통해 일부만 관찰가능하며, 
플레이어는 정보를 얻기위해 적극적으로 움직여야 한다. 
더욱이, 지도에서 정찰하지 않은 곳이 흐리게 표현되는 전장안개(fog-of-war)개념은 
적의 상태(state)를 판단하기 위해 플레이어의 적극적인 움직임을 요구한다.
셋째, 활동공간(action space) 는 광활하게 넓으며 그 종류가 다양하다. 
플레이어는 대략 $ 10^8 $ 개로 조합된 공간 중 하나의 행동을 선택한다.  
그리고 많은 종류의 서로 다른 유닛과 건물 종류가 있는데, 각각은 유니크한 행동으로 이어지며, 여러 종류의 빌드오더가 존재한다. 
넷째, 이 게임은 수천개의 프레임과 행동이 지속되고 어떤 유닛을 생산할 것인지 이른 판단(early decision)이 요구되며,
적을 발견하기 전까진 적을 볼 수 없다. 이것은 시간제약적 신뢰할당(temporal credit assignment)과 탐색(exploration)으로 이어진다.
<br /> 
본 논문은 스타크래프트 강화학습을 수월하게 해주는 인터페이스를 소개한다. 
* 피여의 낮은 해상도 그리드를 이용해 관측과 행동이 정의 된다. 
* 보상(reward)은 스타크래프트2 엔진을 기반으로 컴퓨터 적을 통해 정해진다.
* 간소화된 미니게임 또한 전체 게임맵으로 제공되고 이는 스타크래프트2 전체 게임의 인터페이스로 확장가능하며 관측과 행동이 RGB 픽셀로 드러난다.
* 에이전트는 멀티플레이어 게임에서 마지막 승리 혹은 패배로 순위가 결정된다
* 평가는 실제 사람과 경쟁하기 위해 전체 게임맵으로 제한 된다. 

게다가 우리는 실제 사람의 리플레이에 기반한 대량의 데이터셋을 제공하는데 이는 실제 사람이 수백만번 플레이한 것과 같다. 
우리는 인터페이스 조합과 이 데이터셋이 기존에 존재하거나 새로운 강화학습 알고리즘을 테스트하기 위한 유용한 벤치마크를 제공할 것이라 믿을뿐만아니라,
퍼셉트론, 메모리와 attention, 순차적 예측(sequence prediction), 불확실성 모델링 등 관점에서 흥미로운 측면이 존재하는데 
이는 모두 머신러닝 연구가 활발한 분야이다.
<br />
몇몇 환경에서 이미 스타크래프트1 강화학습이 존재한다.
우리 연구는 이전의 환경들과는 다소 다른 측면이 있다.
* 우리는 스타크래프트1이 아닌 스타크래프트2를 이용한다.
* 관측과 행동은 프로그램적이라기 보단, 실제 인간 유저 인터페이스에 기반한다.
* 해당 게임개발사인 블리자드 엔터테인먼트의 서포트를 받는다.
이전 연구 환경에 기반한 현존 최강 스타크래프트 인공지능이라 할지라도 실제 인간 아마추어조차 이길 수 없었다. 
이런면에서 보면, 스타크래프트는 흥미로운 게임 플레이 특성과 넓은 유저 기반이 심층 강화학습 연구를 가능하게 한다. 
<br />

## 2. 관련 연구
컴퓨터 게임은 인공지능(AI) 연구의 중요한 자원이 될 뿐 만 아니라,
여러 가지 평가, 서로 다른 학습 방법 비교, 표준화된 업무 계획 이슈에 대해 주목할 만한 해결책을 제공한다.또한 
또한 여러 가지 장점을 포함하는데,
* 성공이라는 개념이 객관적이고 측정 가능한 점.
* 컴퓨터 게임 특유의 관측 가능한 데이터를 통한 전형적인 아웃풋 흐름.
* 인간이 플레이하기 어려운 난이도는 연구자가 알고리즘을 개발하여 문제를 쉽게 만들어 튜닝하는 것이 아닌 인공지능 그 자체가 가능하도록 한다.
* 게임은 동일한 인터페이스, game dynamics, 실행되도록 디자인되어 있는데 이것은 다른 연구자들과 쉽게 공유 가능하게 한다.
* 흔히 고인물이라 불리는 고수 플레이어가 모여있는 곳이 존재하는데, 이는 높은 기술 수준의 개인을 벤치마크 가능케한다.
* 게임은 시뮬레이션이기 때문에, 컨트롤을 정확하게 할 수 있으며, 규모를 고려해 실행할 수 있다.

![Figure1](/assets/images/sc2le/figure1.JPG)

그림 1: 스타크래프트2 학습 환경(SC2LE)의 플러그인된 뉴럴 에이전트 요소를 보여주는 그림.
<br />

아마도 강화학습연구를 이끄는 가장 좋은 예는 아타리게임을 이용해 쉽고 반복가능한 실험이 가능케하는 Arcade Learning Envoronment(ALE) 일 것이다.
이러한 표준화된 과제는 최근 AI 연구에서 대단히 유용하다.
ALE환경에서 이용되는 게임 스코어는 논문이나 알고리즘간에 비교될수 있고, 직접적인 관측 및 비교를 가능케한다.
ALE는 슈퍼마리오, 팩맨, 둠, 언리얼토너먼트와 같은 유서 깊은 비디오 게임 AI 벤치마크의 좋은 예일 뿐 아니라 일반적인 비디오 게임에도 적용된다.
<br />

스타크래프트1(브루드워)과 같은 RTS는 인공지능을 연구하기에 매력적인 장르이다. 
우리는 Ontanon의 서베이 결과, Robertson & Watson의 어버뷰를 추천한다.
이와 같은 많은 연구 결과들는 게임의 특정 면(ex: 빌드오더, 마이크로 컨트롤)에 집중하거나 특정 AI 테크닉(ex: MCTS planning)에 집중한다.
우리는 전체 게임을 풀어나가는 end-to-end RL 접근방법을 알지 못한다.
다루기힘든 RTS의 풀버전게임은 엄청난 양의 인풋, 아웃풋 공간 뿐 아니라, sparse reward structure(ex: 게임 결과)으로 인해 해결하기 벅차 보인다.
<br />

일반적인 스타크래프트 API는 BWAPI와 멀어지고 wrappers와 연관된다.
AI 연구에 의해 RTS의 간소화된 버전이 개발되었는데, 특히 microRTS, 최근에는 ELF가 주목할만하다.
또한 학습기반 에이전트는 리플레이 데이터를 통해 micro-management mini-game을 explore 하거나 게임 결과를 학습하고, 빌드오더를 학습한다.
<br />

## 3.스타크래프트2 가상환경
