---
title: "[번역]StarCraft2 A New Challenge for Reinforcement Learning"
categories:
  - PaperTranslation
tags:
  - ReinforcementLearning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
---

# 스타크래프트2 AI 환경설정
원문 : StarCraft2 A New Challenge for Reinforcement Learning

<br />

## 요약  
이 논문은 스타크래프트2를 기반으로 하는 강화 학습 환경을 소개한다.
이는 여러 면에서 기존 강화학습에 비해 새롭고 광활한 뿐 아니라, 더욱 도전적이다. 

* 다수의 유저가 상호작용하는 멀티-에이전트 문제
* 일부만 관찰된 맵에 의한 불완전한 정보
* 수백 개의 유닛에 대한 선택과 컨트롤을 수반하는 광범위한 행동공간(action space) 
* raw input 피쳐 평면으로 인한 넓은 상태공간(state space)
* 수천번의 단계 동안 장기전략(long-term strategies)이 요구되는 신뢰할당(credit assignment) 딜레이 문제.

우리는 스타크래프트2에 대해 관측(observation), 행동(action), 보상(reward)을 설명하고, 
게임엔진과 의사소통하는 파이썬 베이스 인터페이스를 오픈소스로 제공한다.
메인 게임 맵 뿐 만 아니라, 우리는 스타크래프트2의 서로 다른 요소에 집중하는 미니 게임을 제공한다.
메인 게임 맵에 대해선, 우리는 실제 고수 유저의 리플레이 데이터셋을 제공한고,
이 데이터를 이용해 신경망을 이용하여 학습시킨 초기 베이스라인이 되는 결과 또한 제공한다.
이는 게임 결과와 유저의 행동 예측에 사용된다. 
즉, 스타크래프트2에 적용되는 표준(canonical) 심층강화학습 에이전트로 사용된다는 뜻이다.
미니게임에서의 플레이 목표 수준은 초보 유저와 비교할수 있을 정도이다.
그러므로, 스타크래프트2 강화 학습 환경은 심층강화학습 알고리즘과 아키텍쳐에 대해 새롭운 도전을 제공한다.
<br />
 
## 1.인트로
최근 발전하고 있는 음성인식, 컴퓨터 비전, 자연어처리는 신경망을 이용해 비선형 점근 함수를 구하기 위한 
강력한 툴을 제공함으로써 딥러닝의 부활에 기여했다. 또한 이러한 기술은 아타리 게임, 바둑, 3차원 가상 환경과 로보틱스 분야에서
유의미한 성공을 보이는 강화학습에서도 성공적이라는 것이 입증되었다. 이러한 큰 성공은 어려운 난이도의 분야에서도 시뮬레이션 되었다.
이러한 벤치마크는 진보된 딥러닝과 강화학습 연구결과에 중요한 기여를 하였다. 
이는 1차원 이상에서 현재 방법의 역량을 뛰어넘는 도메인의 유효성을 확실히 하는 면에서 대단히 중요하다.
본 논문에서는 새롭고 도전적인 강화학습 도메인인 스타크래프트2라는 게임의 SC2LE(스타크래프트2 학습환경)을 소개한다.
스타크래프트는 실시간 전략 시뮬레이션(RTS) 게임인데, 빠른 micro-action과 높은 수준의 전략수립과 실행능력을 요구한다. 
지난 20년 동안, 스타크래프트1과 2는 e-sports의 개척자였으며, 수많은 스타 플레이어를 탄생시켰다.
따라서 최고 수준의 플레이어를 이기는 것은 의미있으며 장기적인 목표가 될 수 있다.
<br />
강화학습 관점으로 스타크래프트2는 기존의 한계를 뛰어넘을 만한 최고의 기회를 제공한다. 
첫째, 스타크래프트2는 여러 명의 플레이어가 자원을 놓고 경쟁하는 멀티에이전트 문제이다.
또한 각 플레이어들은 공통 목표를 달성하기 위해 수백개의 유닛을 컨트롤 하며 협동하는 lower-level 멀티에이전트라고 볼 수 있다. 
둘째, 스타크래프트2는 불완전한 정보 게임이다. 맵은 오직 로컬 카메라를 통해 일부만 관찰가능하며, 
플레이어는 정보를 얻기위해 적극적으로 움직여야 한다. 
더욱이, 지도에서 정찰하지 않은 곳이 흐리게 표현되는 전장안개(fog-of-war)개념은 
적의 상태(state)를 판단하기 위해 플레이어의 적극적인 움직임을 요구한다.
셋째, 활동공간(action space) 는 광활하게 넓으며 그 종류가 다양하다. 
플레이어는 대략 $10^8$ 개로 조합된 공간 중 하나의 행동을 선택한다.  
그리고 많은 종류의 서로 다른 유닛과 건물 종류가 있는데, 각각은 유니크한 행동으로 이어지며, 여러 종류의 빌드오더가 존재한다. 
넷째, 이 게임은 수천개의 프레임과 행동이 지속되고 어떤 유닛을 생산할 것인지 이른 판단(early decision)이 요구되며,
적을 발견하기 전까진 적을 볼 수 없다. 이것은 시간제약적 신뢰할당(temporal credit assignment)과 탐색(exploration)으로 이어진다.
<br /> 
본 논문은 스타크래프트 강화학습을 수월하게 해주는 인터페이스를 소개한다. 
* 피여의 낮은 해상도 그리드를 이용해 관측과 행동이 정의 된다. 
* 보상(reward)은 스타크래프트2 엔진을 기반으로 컴퓨터 적을 통해 정해진다.
* 간소화된 미니게임 또한 전체 게임맵으로 제공되고 이는 스타크래프트2 전체 게임의 인터페이스로 확장가능하며 관측과 행동이 RGB 픽셀로 드러난다.
* 에이전트는 멀티플레이어 게임에서 마지막 승리 혹은 패배로 순위가 결정된다
* 평가는 실제 사람과 경쟁하기 위해 전체 게임맵으로 제한 된다. 

게다가 우리는 실제 사람의 리플레이에 기반한 대량의 데이터셋을 제공하는데 이는 실제 사람이 수백만번 플레이한 것과 같다. 
우리는 인터페이스 조합과 이 데이터셋이 기존에 존재하거나 새로운 강화학습 알고리즘을 테스트하기 위한 유용한 벤치마크를 제공할 것이라 믿을뿐만아니라,
퍼셉트론, 메모리와 attention, 순차적 예측(sequence prediction), 불확실성 모델링 등 관점에서 흥미로운 측면이 존재하는데 
이는 모두 머신러닝 연구가 활발한 분야이다.
<br />
몇몇 환경에서 이미 스타크래프트1 강화학습이 존재한다.
우리 연구는 이전의 환경들과는 다소 다른 측면이 있다.
* 우리는 스타크래프트1이 아닌 스타크래프트2를 이용한다.
* 관측과 행동은 프로그램적이라기 보단, 실제 인간 유저 인터페이스에 기반한다.
* 해당 게임개발사인 블리자드 엔터테인먼트의 서포트를 받는다.

이전 연구 환경에 기반한 현존 최강 스타크래프트 인공지능이라 할지라도 실제 인간 아마추어조차 이길 수 없었다. 
이런면에서 보면, 스타크래프트는 흥미로운 게임 플레이 특성과 넓은 유저 기반이 심층 강화학습 연구를 가능하게 한다. 
<br />

## 2. 관련 연구

컴퓨터 게임은 인공지능(AI) 연구의 중요한 자원이 될 뿐 만 아니라,
여러 가지 평가, 서로 다른 학습 방법 비교, 표준화된 업무 계획 이슈에 대해 주목할 만한 해결책을 제공한다.또한 
또한 여러 가지 장점을 포함하는데,
* 성공이라는 개념이 객관적이고 측정 가능한 점.
* 컴퓨터 게임 특유의 관측 가능한 데이터를 통한 전형적인 아웃풋 흐름.
* 인간이 플레이하기 어려운 난이도는 연구자가 알고리즘을 개발하여 문제를 쉽게 만들어 튜닝하는 것이 아닌 인공지능 그 자체가 가능하도록 한다.
* 게임은 동일한 인터페이스, game dynamics, 실행되도록 디자인되어 있는데 이것은 다른 연구자들과 쉽게 공유 가능하게 한다.
* 흔히 고인물이라 불리는 고수 플레이어가 모여있는 곳이 존재하는데, 이는 높은 기술 수준의 개인을 벤치마크 가능케한다.
* 게임은 시뮬레이션이기 때문에, 컨트롤을 정확하게 할 수 있으며, 규모를 고려해 실행할 수 있다.

![Figure1](/assets/images/sc2le/figure1.JPG)

그림 1: 스타크래프트2 학습 환경(SC2LE)의 플러그인된 뉴럴 에이전트 요소를 보여주는 그림.
<br />

아마도 강화학습연구를 이끄는 가장 좋은 예는 아타리게임을 이용해 쉽고 반복가능한 실험이 가능케하는 Arcade Learning Envoronment(ALE) 일 것이다.
이러한 표준화된 과제는 최근 AI 연구에서 대단히 유용하다.
ALE환경에서 이용되는 게임 스코어는 논문이나 알고리즘간에 비교될수 있고, 직접적인 관측 및 비교를 가능케한다.
ALE는 슈퍼마리오, 팩맨, 둠, 언리얼토너먼트와 같은 유서 깊은 비디오 게임 AI 벤치마크의 좋은 예일 뿐 아니라 일반적인 비디오 게임에도 적용된다.
<br />

스타크래프트1(브루드워)과 같은 RTS는 인공지능을 연구하기에 매력적인 장르이다. 
우리는 Ontanon의 서베이 결과, Robertson & Watson의 어버뷰를 추천한다.
이와 같은 많은 연구 결과들는 게임의 특정 면(ex: 빌드오더, 마이크로 컨트롤)에 집중하거나 특정 AI 테크닉(ex: MCTS planning)에 집중한다.
우리는 전체 게임을 풀어나가는 end-to-end RL 접근방법을 알지 못한다.
다루기힘든 RTS의 풀버전게임은 엄청난 양의 인풋, 아웃풋 공간 뿐 아니라, sparse reward structure(ex: 게임 결과)으로 인해 해결하기 벅차 보인다.
<br />

일반적인 스타크래프트 API는 BWAPI와 멀어지고 wrappers와 연관된다.
AI 연구에 의해 RTS의 간소화된 버전이 개발되었는데, 특히 microRTS, 최근에는 ELF가 주목할만하다.
또한 학습기반 에이전트는 리플레이 데이터를 통해 micro-management mini-game을 explore 하거나 게임 결과를 학습하고, 빌드오더를 학습한다.
<br />

## 3.스타크래프트2 가상환경

SC2LE 릴리즈를 통한 본 논문의 가장 큰 기여는 스타크래프트2를 연구 환경으로 노출시키는 것이다. 
릴리즈는 리눅스 스타크래프트2 바이너리, 스타크래프트2 API, PySC2로 구성되어있다.
<br />

스타크래프트2 API는 프로그램적인 컨트롤을 허용한다. 
API는 게임 시작, 관측, 행동 취하기, 리플레이 리뷰에 활용된다.
노멀 게임에서의 API는 윈도우와 맥OS에서는 사용가능 한 반면, 
리눅스에서는 특히 머신러닝과 분산 처리 케이스에 대해 제한적이고 근본없는 빌드를 제공한다.
<br />

API를 이용하여 우리는 PySC2를 개발했는데, 이것은 RL 에이전트에 대해 최적화된 오픈 소스 환경이다.
PySC2는 스타크래프트2 API를 커버하는 파이썬 환경인데, 이는 파이썬 강화학습 에이전트와 스타크래프트2간의 상호작용을 쉽게끔한다.
PySC2는 행동과 관측을 정의하며, 랜덤 에이전트와 스크립트 에이전트의 다루기 힘듦을 포함한다.
그것은 또한 미니게임과 에이전트가 보고, 할수 있는 것을 파악하기 위해 시각화툴을 포함한다.  
<br />

스타크래프트2는 초당 시뮬레이션 16(at normal speed), 22.4(at fast speed)를 업데이트했다.
이 게임은 확정적이지만, 다소 허울뿐인 랜덤성을 가지고 있다.
두가지 메인 랜덤 요소는 무기속도와 업데이트 순서이다. 
이러한 랜덤성은 랜덤시드 셋팅을 통해 제거되거나 완화시킬수 있다.
<br />

자, 이제 이 논문에서 이용하는 환경을 알아보도록하자.
<br />

### 3.1 전체 게임 묘사와 보상 구조

스타크래프트2 1vs1 게임에서, 양쪽 진영은 자원, 경사로, 입구, 섬과같은 요소들과 함께 시작한다. 
게임에서 승리하기 위해 유저가 해야할 것은 

* 자원(미네랄, 가스)을 축적
* 생산 건물을 짓고,
* 군대를 모으고,
* 상대방 건물을 전멸 시킨다.

한 게임은 보통 몇분에서 길면 한시간까지 이어지고,
게임 초반에 선택하는 행동들, 어떤 건물이나 유닛을 생산할 지는 장기적 결과를 가져온다.
플레이어는 자신의 유닛이 존재하는 부분만 맵으로 확인할 수 있기 때문에 불완전한 정보를 가지고 진행한다.
만약 상대방 전략을 파악하고 싶다면 플레이어는 자신의 유닛을 상대방쪽으로 정찰 보내야한다. 
섹션 후반에 설명하겠지만, 행동공간은 꽤 유니크하고 도전적이다.
<br />

대부분의 플레이어는 온라인을 통해 다른 사람을 상대로 플레이한다. 
그 중 가장 인기있는 것은 1v1게임이지만 2v2, 3v3, 4v4 같은 팀플레이도 가능하며 두 팀 이상끼리 싸우는 것도 가능하다.
이 중 우리는 스타크래프트에서 가장 유명한 1v1 게임에 집중할 것이지만, 향후 더욱 복잡한 상황까지 고려할 것이다.
<br />

스타크래프트2는 자체 AI를 가지고 있는데 이 자체 AI는 손수 제작된 규칙의 집합이고, 
총 10단계의 난이도를 가지고 있다.(가장 강력한 세 단계는 자원을 더 많이 가지고 시작하거나 맵 전체를 볼 수 있는 기능으로 무장되어있다.) 
불행히도, 자체 AI의 전략의 선택폭은 좁은 편이다. 
쉽게 말해서, 자체 AI는 쉽게 이길 수 있으므로, 플레이어는 쉽게 흥미를 잃게 된다. 
그럼에도 불구하고, 자체 AI는 베이스라인에서 시작할 때(section4, 5에서 설명), 첫 번째 도전자로서 꽤 합당하다. 
그들은 막무가내로 플레이하지 않으며, 적은 계산량으로 플레이하고, 상대방과 비교했을때 일관적인 베이스라인을 제공한다. 
<br />

우리는 두가지 보상 구조(reward structure)를 정의한다. 
셋으로 이루어진 1(win) / 0(tie) -1(loss) 는 게임이 끝나고 블리자드 스커어와 함께 얻게 된다.
이 보상은 게임 플레이 동안에는 쭉 0점이다. 
이 승리/무승부/패배 점수는 우리가 관심있는 실제 보상이다. 
원래 블리자드스코어는 게임이 끝난 후 플레이어 스크린에서 확인 가능한데,
우리는 실행 중인 게임에서 각 스텝마다 강화학습의 보상으로 이용되는 블리자드스코어에 항상 접근 가능하다. 
그것은 현재 자원의 합과 업그레이드 상황, 현재 살아있는 유닛과 건물을 이용해 계산된다. 
즉, 플레이어의 누적된 보상은 자신의 자원을 통해 증가하고, 유닛이나 건물을 잃을 때 감소하며, 
그 외에 생산중인 유닛, 건설중인 거물, 업그레이드 중인 업그레이드 같은 요인들은 보상에 영향을 미치지 않는다. 
블리자드스코어는 플레이어 중심적이므로 제로섬이 아니며, 승리/무승부/패배 점수에 비해 덜 sparse 하다. 
또한 승리나 패배와 상관관계가 있다. 
<br />

### 3.2 관측 

스타크래프트2는 3D 렌더링 그래픽 게임엔진을 이용한다. 
전체 환경을 시뮬레이션 하는 게임엔진을 사용하는 반면, 스타크래프트2 API는 RGB 픽셀을 렌더링 할 수 없다.
그러나 사람이 플레이하는 동안 RGB 이미지를 추상적인 방법으로 feature layers는 만들수 있고,
이것은 스타크래프트2의 핵심 공간적, 그래픽적 개념을 표현할 수 있다.(Figure2 참고)
<br />

![Figure2](/assets/images/sc2le/figure2.JPG)

Figure2: PySC2 뷰어는 사람이 해석 가능한 뷰를 왼쪽에서 보여주고, 피쳐레이어의 칼러 버전을 오른쪽에서 보여준다.
예를 들어, 지형 높이, 전장안개, 크립, 카메라위치, 플레이어 진영은 피쳐레이어에서 탑 부분에서 볼 수 있다.
해당 영상은 [http://youtu.be/-fKUyT14G-8](http://youtu.be/-fKUyT14G-8) 에서 볼 수 있다.
<br />

그러므로, 피쳐레이어의 메인 관측은 $N \times M$ 픽셀로 나타나며 $N = M$ 으로 설정 가능하다.
각각의 레이어는 인게임에서 특별함을 나타낸다. 예를들어 유닛 타입이나, 히트 포인트, 소유자, 가시성이 그것이다. 
이 중에는 스칼라로 표현되는 것(히트 포인트, 지형 높이)도 있고, 카테고리로 표현되는 것(가시성, 유닛타입, 소유자)도 있다. 
피쳐레이어에는 미니맵과 스크린 두가지 셋이 있다. 
미니맵은 전체 판을 대략적인 표현으로 나타내고, 
스크린은 플레이어가 스크린에서 볼 수 있고 행동을 실행할 수 있는 전체 판의 일부분을 세부적인 뷰를 통해 확인 가능하게 한다.  
소유자나 가시성 같은 피쳐들은 스크린과 미니맵 두가지 모두에서 확인 가능하고, 유닛 타입이나 히트 포인트는 오직 스크린에서만 확인 가능하다.
모든 관측이 제공되는 환경 문서를 참고하기 바란다.
<br />

스크린과 미니맵 뿐 만 아니라, 휴먼 인터페이스는 다양한 비공간(non-spatial) 관측을 제공한다.
미네랄이나 가스가 얼마나 모였는지, 현재 이용 가능한 행동 집합(물론 이것은 게임 상황에 따라 달라진다), 
선택한 유닛의 자세한 정보, 예정된 빌드, 수송선에 실려있는 유닛 등을 제공한다. 
이러한 관측은 PySC2에서 또한 확인 가능하며, 환경 문서에 소개되어있다.
<br />

스크린의 전체 게임은 고해상도 풀 3D 카메라 시점으로 렌더링 되어있다.
이것은 유닛이 스크린에서 높아져서 작아지거나 정면보다 뒤에서 볼 수 있는 세밀한 관측을 가능케한다. 
쉽게 말하면, 피쳐레이어는 카메라를 통해 렌더링 되어있는데, 탑다운 정사영을 사용한다. 
즉, 피쳐레이어의 각 픽셀은 실제 영역과 정확히 같은 양이고, 
결과적으로 그들이 어느 뷰에 있던 상관없이 모든 유닛은 같은 크기가 된다. 
불행히도 이 말은 피쳐레이어 렌더링이 인간이 보는 시야와 일치하지 않는다는 것을 뜻한다. 
에이전트는 뒤(back) 보다는 앞(front)을 많이 보게 된다. 
이것은 리플레이에서 하는 인간의 완벽한 행동 재현을 뜻하지는 않는다.
<br />

향후 릴리즈에서 우리는 RBG 픽셀로 에이전트가 플레이 할 수 있는 렌더링된 API를 제공할 것이다. 
이것은 로우 픽셀, 픽셀레이어로 학습 효과를 연구할 수 있게 하고 실제 유저의 플레이와 더욱 흡사하게 한다. 
그 동안, 우리는 피쳐레이어를 통해 에이전트는 과하게 핸디캡이 주어지지 않았다는 것을 확인하며 게임플레이를 하였다. 
비록 게임 플레이 경험이 명백히 바뀌었는데, 해상도 $N, M \geq 64$ 는 저글링과 같은 작은 유닛을 컨트롤 할 수 있을 정도로 충분하다는 것을 발견했다.
이 글을 읽고있는 독자는 Figure2와 pysc2_play를 통해 직접 체험 해 볼 수 있다. 
<br />

### 3.3 행동

우리는 휴먼 인터페이스를 최대한 가깝게 구현하고자 행동 공간 환경을 디자인 한다.
다른 강화학습 환경에서 아타리 같은 전통적인 방법을 유지하는 반면, 
Figure3은 플레이어와 에이전트의 짧은 행동 순서를 보여준다. 
<br />

![Figure3](/assets/images/sc2le/figure3.JPG)

Figure3: 스타크래프트2에서 실제 유저 행동과 PySC2에서 표현되는 행동의 비교. 
우리는 행동공간을 실제 유저 행동에 최대한 가깝게 디자인하였다. 
첫 번째 행은 게임 스크린을 보여주고 두번째 행은 실제 유저 행동을 보여주며, 
세번째 행은 PySC2에서의 논리적 행동을 보여주고, 
마지막으로 네번째 행에서는 행동 $a$가 PySC2 환경에서 표현되는 것을 보여준다. 
첫 두 열은 'build supply' 행동을 하지 않았다는 것에 주목해라, 
이것은 첫번째 선택한 에이전트에 아직 가능한 상황이 아니다.

인게임에서 많은 기본적인 동작은 복합적인 행동이다. 
예를 들어, 유닛을 맵을 가로질러서 이동시킨다고하면, 
플레이어는 m 키를 누르고 나서 해당 행동을 큐에 넣기위해 shift 키를 누를수 있다.
그러고나서 스크린이나 미니맵에 원하는 지점을 찍을 수 있다. 
에이전트에 세가지 분리된 행동을 시키기 위해  3개의 키보드나 마우스 입력을 요청하는 대신 
아주작은 복합적인 함수 행동을 시킬 수 있다 : $move_screen(queued, screen)$. 
<br />

공식적으로, 행동 $a$는 function identifier $a^0$과 $a^1, a^2,...,a^L$ arguments의 순서 구성으로서 나타낼 수 있다. 
예를 들어, 마우스 드래그를 통해 여러 유닛을 선택하는 상황을 생각해보자. 
의도된 행동은 $select_rect(select_add, (x^1, y^1), (x^2, y^2))$이다. 
첫번째 argument $select_add$는 바이너리이다. 
다른 argument 들은 좌표공간에 정의된 정수이다. 
그들의 범위(range)는 관측의 해상도와 같다. 
이 행동은 환경으로부터 $select_rect, [[select_add],[x^1, y^1],[x^2, y^2]]]$ 형태로 제공된다. 
full 행동공간을 표현하기 위해 우리는 13 종류의 argument를 가진 대략 300개의 행동 함수 식별자를 정의한다.(바이너리부터 불연속 2D 스크린의 특정지점까지)
PySC2를 통한 행동 묘사 및 자세한 사항은 환경 문서를 참고하길 바라며, Figure3는 행동순서의 예이다.
<br />

스타크래프트에서는 모든 게임에서 모든 행동이 가능한 것은 아니다. 
예를 들어, 무브 커맨드는 유닛을 선택했을 때만 가능하다. 
유저는 스크린에서 어떤 행동이 가능한지 볼 수 있다. 
이와 비슷하게, 에이전트의 각 스텝에서 관측을 통해 행동가능 리스트를 제공한다. 
불가능한 행동을 취하는 것이 에러로 인식되므로 에이전트는 행동가능리스트 중에 행동을 선택하게 된다. 
<br />

실제 유저는 30 ~ 300 APM(손빠르기)를 가지는 데, 
유저의 스킬에 따라 증가하는 경향이 있으며, 프로게이머의 경우 500 APM을 넘는 경우도 있다. 
우리 실험에서는 프레임당 8가지 행동을 하게되는데 이는 180 APM에 해당하며 이는 중수 정도에 해당한다.
<br />

이렇게 미리 환경을 디자인하는 것은 더욱 복잡한 RL 에이전트를 개발하는데 도움이 된다. 
특히, 픽셀사이즈 피쳐 레이어 인풋 공간과 인간같은 행동 공간은 에이전트 기반 뉴럴 네트워크를 사용하기 자연스럽다. 
이것은 다른 최근 논문과는 반대되는 사항인데, 최근 논문들은 유닛 하나하나에 기반하며 행동들 또한 유닛별로 따로 접근한다. 
각각의 인터페이스 스타일에는 장점이있는 반면, PySC2의 장점은 다음과 같다.

* 실제 유저 리플레이를 통한 학습은 다른 스타일에 비해 간단하다.
* 비현실적/인간을 뛰어넘는 APM을 요구하지 않는다.
* 이러한 UI로 디자인된 게임과 높은 수준의 결정, 경제 관리, 병력 컨트롤 사이의 밸런스는 게임을 더욱 흥미롭게 만든다.

### 3.4 미니게임 묘사

풀 게임 플레이에서 고립된 상황에서의 요인을 조사 하고 더욱 부드러운 스텝을 제공하기 위해, 우리는 몇몇 미니게임을 개발했다.
미니게임은 명확한 보상구조를 가지고 있는 몇가지 행동이나 게임기법을 위한 테스트 목적으로 제작된 작은 맵에서 시나리오에 집중한다. 
보상이 승리/패배/무승부인 풀 게임과는 달리, 미니게임의 보상구조는 특별한 행동이다.(SC2Map 파일에 정의 되어있다) 
<br />

우리는 커뮤니티에 강력한 스타크래프트 맵에디터를 이용한 빌드 수정이나 새로운 미니게임을 하도록 권장했다. 
이것은 단지 작은 영역에서 도전을 계획 한 것 이상을 가능케했다. 
그것은 서로다른 연구자에게 동일한 설정과 평가, 평가 점수의 직접적인 비교가 가능하도록 했다.
제한된 행동 셋은 커스텀 보상 함수와 시간 제약이 쉽게 공유 가능하도록 .SC2Map 파일에 결과에 직접적으로 정의 되어 있다. 
그러므로 우리는 유저가 에이전트쪽에 커스터마이징된 것보다 새롭게 정의된 업무를 이용하는 것을 권장한다. 
<br />

다음은 우리가 릴리징한 일곱가지 미니게임이다.

* MoveToBeacon: 에이전트는 마린 1기로 시작하는데 비콘에 도착할 때마다 마린 숫자가 하나씩 늘어난다. 
이 맵은 trivial greedy stragegy 유닛 테스트이다. 
* CollectMineralShards: 에이전트는 마린 2기로 시작하고 그들을 선택해서 맵 곳곳에 퍼져있는 미네랄로 이동시킨다. 
좀 더 효율적으로 움직일수록 높은 스코어를 기록한다. 
* FindAndDefeatZerglings: 에이전트는 마린 3기로 시작하고 맵을 탐험하여 저글링을 격파해야한다. 
이것은 카메라 움직임과 효율적인 탐험을 요구한다. 
* DefeatRoaches: 에이전트는 마린 9기로 시작하고 바퀴 4기를 격파해야한다. 
바퀴를 모두 격파시키면 5기의 마린이 추가되고 4기의 바퀴가 생성된다. 
바퀴하나당 보상은 +10에 해당하고 마린이 하나 죽을 때마다 보상은 -1이다. 
마린을 죽지 않고 잘 관리해야하고 바퀴를 전멸시켜야한다. 
* DefeatZerglingsAndBanelings: 적이 저글링과 맹독충이라는 사실을 제외하곤 바퀴 미니게임과 동일하다. 
상대방을 죽일때마다 +5의 보상이 주어진다. 이 게임은 상대방이 서로 다른 능력을 가지고 있으므로 다른 전략이 요구 된다. 
* CollectMineralsAndGas: 에이전트는 제한된 베이스로 시작하고 제한된 시간에서 자원 수급으로 보상이 결정된다. 
성공적인 에이전트는 일꾼을 더 많이 생산하여 자원 수급률을 증가시킨다. 
* BuildMarines: 에이전트는 제한된 베이스로 시작하고 마린을 생산함으로써 보상을 받는다. 
이를 위해 일꾼을 생산하고 자원을 수급하고 서플라이 디폿, 배럭을 건설해야하며 그 이후 마린을 생산해야한다. 
이 목표를 달성하기 위해 최소한의 행동 셋으로 행동 공간을 제약시킨다.

모든 미니게임은 시간 제한이 정해져 있고 더 자세한 사항은 [https://github.
com/deepmind/pysc2/blob/master/docs/mini_games.md](https://github.
com/deepmind/pysc2/blob/master/docs/mini_games.md)을 참고하길 바란다.
<br />

### 3.5 Raw API




  

