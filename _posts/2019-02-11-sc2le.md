---
title: "[번역]StarCraft2 A New Challenge for Reinforcement Learning"
categories:
  - PaperTranslation
tags:
  - ReinforcementLearning
use_math: true
---

# 스타크래프트2 AI 환경설정
원문 : StarCraft2 A New Challenge for Reinforcement Learning

<br />

## 요약  
이 논문은 스타크래프트2를 기반으로 하는 강화 학습 환경을 소개한다.
이는 여러 면에서 기존 강화학습에 비해 새롭고 광활한 뿐 아니라, 더욱 도전적이다. 

* 다수의 유저가 상호작용하는 멀티-에이전트 문제
* 일부만 관찰된 맵에 의한 불완전한 정보
* 수백 개의 유닛에 대한 선택과 컨트롤을 수반하는 광범위한 행동공간(action space) 
* raw input 피쳐 평면으로 인한 넓은 상태공간(state space)
* 수천번의 단계 동안 장기전략(long-term strategies)이 요구되는 신뢰할당(credit assignment) 딜레이 문제.

우리는 스타크래프트2에 대해 관측(observation), 행동(action), 보상(reward)을 설명하고, 
게임엔진과 의사소통하는 파이썬 베이스 인터페이스를 오픈소스로 제공한다.
메인 게임 맵 뿐 만 아니라, 우리는 스타크래프트2의 서로 다른 요소에 집중하는 미니 게임을 제공한다.
메인 게임 맵에 대해선, 우리는 실제 고수 유저의 리플레이 데이터셋을 제공한고,
이 데이터를 이용해 신경망을 이용하여 학습시킨 초기 베이스라인이 되는 결과 또한 제공한다.
이는 게임 결과와 유저의 행동 예측에 사용된다. 
즉, 스타크래프트2에 적용되는 표준(canonical) 심층강화학습 에이전트로 사용된다는 뜻이다.
미니게임에서의 플레이 목표 수준은 초보 유저와 비교할수 있을 정도이다.
그러므로, 스타크래프트2 강화 학습 환경은 심층강화학습 알고리즘과 아키텍쳐에 대해 새롭운 도전을 제공한다.
<br />
 
## 1.인트로
최근 발전하고 있는 음성인식, 컴퓨터 비전, 자연어처리는 신경망을 이용해 비선형 점근 함수를 구하기 위한 
강력한 툴을 제공함으로써 딥러닝의 부활에 기여했다. 또한 이러한 기술은 아타리 게임, 바둑, 3차원 가상 환경과 로보틱스 분야에서
유의미한 성공을 보이는 강화학습에서도 성공적이라는 것이 입증되었다. 이러한 큰 성공은 어려운 난이도의 분야에서도 시뮬레이션 되었다.
이러한 벤치마크는 진보된 딥러닝과 강화학습 연구결과에 중요한 기여를 하였다. 
이는 1차원 이상에서 현재 방법의 역량을 뛰어넘는 도메인의 유효성을 확실히 하는 면에서 대단히 중요하다.
본 논문에서는 새롭고 도전적인 강화학습 도메인인 스타크래프트2라는 게임의 SC2LE(스타크래프트2 학습환경)을 소개한다.
스타크래프트는 실시간 전략 시뮬레이션(RTS) 게임인데, 빠른 micro-action과 높은 수준의 전략수립과 실행능력을 요구한다. 
지난 20년 동안, 스타크래프트1과 2는 e-sports의 개척자였으며, 수많은 스타 플레이어를 탄생시켰다.
따라서 최고 수준의 플레이어를 이기는 것은 의미있으며 장기적인 목표가 될 수 있다.
<br />
강화학습 관점으로 스타크래프트2는 기존의 한계를 뛰어넘을 만한 최고의 기회를 제공한다. 
첫째, 스타크래프트2는 여러 명의 플레이어가 자원을 놓고 경쟁하는 멀티에이전트 문제이다.
또한 각 플레이어들은 공통 목표를 달성하기 위해 수백개의 유닛을 컨트롤 하며 협동하는 lower-level 멀티에이전트라고 볼 수 있다. 
둘째, 스타크래프트2는 불완전한 정보 게임이다. 맵은 오직 로컬 카메라를 통해 일부만 관찰가능하며, 
플레이어는 정보를 얻기위해 적극적으로 움직여야 한다. 
더욱이, 지도에서 정찰하지 않은 곳이 흐리게 표현되는 전장안개(fog-of-war)개념은 
적의 상태(state)를 판단하기 위해 플레이어의 적극적인 움직임을 요구한다.
셋째, 활동공간(action space) 는 광활하게 넓으며 그 종류가 다양하다. 
플레이어는 대략 $10^8$ 개로 조합된 공간 중 하나의 행동을 선택한다.  
그리고 많은 종류의 서로 다른 유닛과 건물 종류가 있는데, 각각은 유니크한 행동으로 이어지며, 여러 종류의 빌드오더가 존재한다. 
넷째, 이 게임은 수천개의 프레임과 행동이 지속되고 어떤 유닛을 생산할 것인지 이른 판단(early decision)이 요구되며,
적을 발견하기 전까진 적을 볼 수 없다. 이것은 시간제약적 신뢰할당(temporal credit assignment)과 탐색(exploration)으로 이어진다.
<br /> 
본 논문은 스타크래프트 강화학습을 수월하게 해주는 인터페이스를 소개한다. 
* 피여의 낮은 해상도 그리드를 이용해 관측과 행동이 정의 된다. 
* 보상(reward)은 스타크래프트2 엔진을 기반으로 컴퓨터 적을 통해 정해진다.
* 간소화된 미니게임 또한 전체 게임맵으로 제공되고 이는 스타크래프트2 전체 게임의 인터페이스로 확장가능하며 관측과 행동이 RGB 픽셀로 드러난다.
* 에이전트는 멀티플레이어 게임에서 마지막 승리 혹은 패배로 순위가 결정된다
* 평가는 실제 사람과 경쟁하기 위해 전체 게임맵으로 제한 된다. 

게다가 우리는 실제 사람의 리플레이에 기반한 대량의 데이터셋을 제공하는데 이는 실제 사람이 수백만번 플레이한 것과 같다. 
우리는 인터페이스 조합과 이 데이터셋이 기존에 존재하거나 새로운 강화학습 알고리즘을 테스트하기 위한 유용한 벤치마크를 제공할 것이라 믿을뿐만아니라,
퍼셉트론, 메모리와 attention, 순차적 예측(sequence prediction), 불확실성 모델링 등 관점에서 흥미로운 측면이 존재하는데 
이는 모두 머신러닝 연구가 활발한 분야이다.
<br />
몇몇 환경에서 이미 스타크래프트1 강화학습이 존재한다.
우리 연구는 이전의 환경들과는 다소 다른 측면이 있다.
* 우리는 스타크래프트1이 아닌 스타크래프트2를 이용한다.
* 관측과 행동은 프로그램적이라기 보단, 실제 인간 유저 인터페이스에 기반한다.
* 해당 게임개발사인 블리자드 엔터테인먼트의 서포트를 받는다.
이전 연구 환경에 기반한 현존 최강 스타크래프트 인공지능이라 할지라도 실제 인간 아마추어조차 이길 수 없었다. 
이런면에서 보면, 스타크래프트는 흥미로운 게임 플레이 특성과 넓은 유저 기반이 심층 강화학습 연구를 가능하게 한다. 
<br />

## 2. 관련 연구

컴퓨터 게임은 인공지능(AI) 연구의 중요한 자원이 될 뿐 만 아니라,
여러 가지 평가, 서로 다른 학습 방법 비교, 표준화된 업무 계획 이슈에 대해 주목할 만한 해결책을 제공한다.또한 
또한 여러 가지 장점을 포함하는데,
* 성공이라는 개념이 객관적이고 측정 가능한 점.
* 컴퓨터 게임 특유의 관측 가능한 데이터를 통한 전형적인 아웃풋 흐름.
* 인간이 플레이하기 어려운 난이도는 연구자가 알고리즘을 개발하여 문제를 쉽게 만들어 튜닝하는 것이 아닌 인공지능 그 자체가 가능하도록 한다.
* 게임은 동일한 인터페이스, game dynamics, 실행되도록 디자인되어 있는데 이것은 다른 연구자들과 쉽게 공유 가능하게 한다.
* 흔히 고인물이라 불리는 고수 플레이어가 모여있는 곳이 존재하는데, 이는 높은 기술 수준의 개인을 벤치마크 가능케한다.
* 게임은 시뮬레이션이기 때문에, 컨트롤을 정확하게 할 수 있으며, 규모를 고려해 실행할 수 있다.

![Figure1](/assets/images/sc2le/figure1.JPG)

그림 1: 스타크래프트2 학습 환경(SC2LE)의 플러그인된 뉴럴 에이전트 요소를 보여주는 그림.
<br />

아마도 강화학습연구를 이끄는 가장 좋은 예는 아타리게임을 이용해 쉽고 반복가능한 실험이 가능케하는 Arcade Learning Envoronment(ALE) 일 것이다.
이러한 표준화된 과제는 최근 AI 연구에서 대단히 유용하다.
ALE환경에서 이용되는 게임 스코어는 논문이나 알고리즘간에 비교될수 있고, 직접적인 관측 및 비교를 가능케한다.
ALE는 슈퍼마리오, 팩맨, 둠, 언리얼토너먼트와 같은 유서 깊은 비디오 게임 AI 벤치마크의 좋은 예일 뿐 아니라 일반적인 비디오 게임에도 적용된다.
<br />

스타크래프트1(브루드워)과 같은 RTS는 인공지능을 연구하기에 매력적인 장르이다. 
우리는 Ontanon의 서베이 결과, Robertson & Watson의 어버뷰를 추천한다.
이와 같은 많은 연구 결과들는 게임의 특정 면(ex: 빌드오더, 마이크로 컨트롤)에 집중하거나 특정 AI 테크닉(ex: MCTS planning)에 집중한다.
우리는 전체 게임을 풀어나가는 end-to-end RL 접근방법을 알지 못한다.
다루기힘든 RTS의 풀버전게임은 엄청난 양의 인풋, 아웃풋 공간 뿐 아니라, sparse reward structure(ex: 게임 결과)으로 인해 해결하기 벅차 보인다.
<br />

일반적인 스타크래프트 API는 BWAPI와 멀어지고 wrappers와 연관된다.
AI 연구에 의해 RTS의 간소화된 버전이 개발되었는데, 특히 microRTS, 최근에는 ELF가 주목할만하다.
또한 학습기반 에이전트는 리플레이 데이터를 통해 micro-management mini-game을 explore 하거나 게임 결과를 학습하고, 빌드오더를 학습한다.
<br />

## 3.스타크래프트2 가상환경

SC2LE 릴리즈를 통한 본 논문의 가장 큰 기여는 스타크래프트2를 연구 환경으로 노출시키는 것이다. 
릴리즈는 리눅스 스타크래프트2 바이너리, 스타크래프트2 API, PySC2로 구성되어있다.
<br />

스타크래프트2 API는 프로그램적인 컨트롤을 허용한다. 
API는 게임 시작, 관측, 행동 취하기, 리플레이 리뷰에 활용된다.
노멀 게임에서의 API는 윈도우와 맥OS에서는 사용가능 한 반면, 
리눅스에서는 특히 머신러닝과 분산 처리 케이스에 대해 제한적이고 근본없는 빌드를 제공한다.
<br />

API를 이용하여 우리는 PySC2를 개발했는데, 이것은 RL 에이전트에 대해 최적화된 오픈 소스 환경이다.
PySC2는 스타크래프트2 API를 커버하는 파이썬 환경인데, 이는 파이썬 강화학습 에이전트와 스타크래프트2간의 상호작용을 쉽게끔한다.
PySC2는 행동과 관측을 정의하며, 랜덤 에이전트와 스크립트 에이전트의 다루기 힘듦을 포함한다.
그것은 또한 미니게임과 에이전트가 보고, 할수 있는 것을 파악하기 위해 시각화툴을 포함한다.  
<br />

스타크래프트2는 초당 시뮬레이션 16(at normal speed), 22.4(at fast speed)를 업데이트했다.
이 게임은 확정적이지만, 다소 허울뿐인 랜덤성을 가지고 있다.
두가지 메인 랜덤 요소는 무기속도와 업데이트 순서이다. 
이러한 랜덤성은 랜덤시드 셋팅을 통해 제거되거나 완화시킬수 있다.
<br />

자, 이제 이 논문에서 이용하는 환경을 알아보도록하자.
<br />

### 3.1 전체 게임 묘사와 보상 구조

스타크래프트2 1vs1 게임에서, 양쪽 진영은 자원, 경사로, 입구, 섬과같은 요소들과 함께 시작한다. 
게임에서 승리하기 위해 유저가 해야할 것은 
* 자원(미네랄, 가스)을 축적
* 생산 건물을 짓고,
* 군대를 모으고,
* 상대방 건물을 전멸 시킨다.
한 게임은 보통 몇분에서 길면 한시간까지 이어지고,
게임 초반에 선택하는 행동들, 어떤 건물이나 유닛을 생산할 지는 장기적 결과를 가져온다.
플레이어는 자신의 유닛이 존재하는 부분만 맵으로 확인할 수 있기 때문에 불완전한 정보를 가지고 진행한다.
만약 상대방 전략을 파악하고 싶다면 플레이어는 자신의 유닛을 상대방쪽으로 정찰 보내야한다. 
섹션 후반에 설명하겠지만, 행동공간은 꽤 유니크하고 도전적이다.
<br />

대부분의 플레이어는 온라인을 통해 다른 사람을 상대로 플레이한다. 
그 중 가장 인기있는 것은 1v1게임이지만 2v2, 3v3, 4v4 같은 팀플레이도 가능하며 두 팀 이상끼리 싸우는 것도 가능하다.
이 중 우리는 스타크래프트에서 가장 유명한 1v1 게임에 집중할 것이지만, 향후 더욱 복잡한 상황까지 고려할 것이다.
<br />

스타크래프트2는 자체 AI를 가지고 있는데 이 자체 AI는 손수 제작된 규칙의 집합이고, 
총 10단계의 난이도를 가지고 있다.(가장 강력한 세 단계는 자원을 더 많이 가지고 시작하거나 맵 전체를 볼 수 있는 기능으로 무장되어있다.) 
불행히도, 자체 AI의 전략의 선택폭은 좁은 편이다. 
쉽게 말해서, 자체 AI는 쉽게 이길 수 있으므로, 플레이어는 쉽게 흥미를 잃게 된다. 
그럼에도 불구하고, 자체 AI는 베이스라인에서 시작할 때(section4, 5에서 설명), 첫 번째 도전자로서 꽤 합당하다. 
그들은 막무가내로 플레이하지 않으며, 적은 계산량으로 플레이하고, 상대방과 비교했을때 일관적인 베이스라인을 제공한다. 
<br />

우리는 두가지 보상 구조(reward structure)를 정의한다. 
셋으로 이루어진 1(win) / 0(tie) -1(loss) 는 게임이 끝나고 블리자드 스커어와 함께 얻게 된다.
이 보상은 게임 플레이 동안에는 쭉 0점이다. 
이 승리/무승부/패배 점수는 우리가 관심있는 실제 보상이다. 
원래 블리자드스코어는 게임이 끝난 후 플레이어 스크린에서 확인 가능한데,
우리는 실행 중인 게임에서 각 스텝마다 강화학습의 보상으로 이용되는 블리자드스코어에 항상 접근 가능하다. 
그것은 현재 자원의 합과 업그레이드 상황, 현재 살아있는 유닛과 건물을 이용해 계산된다. 
즉, 플레이어의 누적된 보상은 자신의 자원을 통해 증가하고, 유닛이나 건물을 잃을 때 감소하며, 
그 외에 생산중인 유닛, 건설중인 거물, 업그레이드 중인 업그레이드 같은 요인들은 보상에 영향을 미치지 않는다. 
블리자드스코어는 플레이어 중심적이므로 제로섬이 아니며, 승리/무승부/패배 점수에 비해 덜 sparse 하다. 
또한 승리나 패배와 상관관계가 있다. 
<br />

### 3.2 관측 

스타크래프트2는 3D 렌더링 그래픽 게임엔진을 이용한다. 
전체 환경을 시뮬레이션 하는 게임엔진을 사용하는 반면, 스타크래프트2 API는 RGB 픽셀을 렌더링 할 수 없다.
그러나 사람이 플레이하는 동안 RGB 이미지를 추상적인 방법으로 feature layers는 만들수 있고,
이것은 스타크래프트2의 핵심 공간적, 그래픽적 개념을 표현할 수 있다.(Figure2 참고)
<br />

![Figure2](/assets/images/sc2le/figure2.JPG)

Figure2: 

  

