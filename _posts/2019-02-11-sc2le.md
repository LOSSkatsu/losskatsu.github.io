---
title: "[번역]StarCraft2 A New Challenge for Reinforcement Learning"
categories:
  - PaperTranslation
tags:
  - ReinforcementLearning
---

# 스타크래프트2 AI 환경설정
원문 : StarCraft2 A New Challenge for Reinforcement Learning

<br />

## 초록
이 논문은 스타크래프트2를 기반으로 하는 강화 학습 환경을 소개한다.
이는 기존 강화학습에 비해 새롭고 광활한 뿐 아니라, 더욱 도전적이다. 

* 다수의 유저가 상호작용하는 멀티-에이전트 문제
* 일부만 관찰된 맵에 의한 불완전한 정보
* 수백 개의 유닛에 대한 선택과 컨트롤을 수반하는 광범위한 행동공간(action space) 
* raw input 피쳐 평면으로 인한 넓은 상태공간(state space)
* 수천번의 단계 동안 장기전략(long-term strategies)이 요구되는 신뢰할당(credit assignment) 딜레이 문제.

우리는 스타크래프트2에 대해 관측(observation), 행동(action), 보상(reward)을 설명하고, 
파이썬을 베이스로 게임엔진과 의사소통하는 인터페이스를 오픈소스로 제공한다.
메인 게임 맵 뿐 만 아니라, 우리는 스타크래프트2의 서로 다른 요소에 집중하는 미니 게임을 제공한다.
메인 게임 맵에 대해선, 우리는 실제 고수 유저의 리플레이 데이터셋을 제공한고,
이 데이터를 이용해 신경망을 이용하여 학습시킨 초기 베이스라인이 되는 결과 또한 제공한다.
이는 게임 결과와 유저의 행동 예측에 사용된다. 
즉, 스타크래프트2에 적용되는 표준(canonical) 심층강화학습 에이전트로 사용된다는 뜻이다.
미니게임에서의 플레이 목표 수준은 초보 유저와 비교할수 있을 정도이다.
그러므로, 스타크래프트2 강화 학습 환경은 심층강화학습 알고리즘과 아키텍쳐에 대해 새롭운 도전을 제공한다.
<br />

## 1.인트로
최근 발전하고 있는 음성인식, 컴퓨터 비전, 자연어처리는 신경망을 이용해 비선형 점근 함수를 구하기 위한 
강력한 툴을 제공함으로써 딥러닝의 부활에 기여했다. 또한 이러한 기술은 아타리 게임, 바둑, 3차원 가상 환경과 로보틱스 분야에서
유의미한 성공을 보이는 강화학습에서도 성공적이라는 것이 입증되었다. 이러한 큰 성공은 어려운 난이도의 분야에서도 시뮬레이션 되었다.
이러한 벤치마크는 진보된 딥러닝과 강화학습 연구결과에 중요한 기여를 하였다. 
이는 1차원 이상에서 현재 방법의 역량을 뛰어넘는 도메인의 유효성을 확실히 하는 면에서 대단히 중요하다.
본 논문에서는 새롭고 도전적인 강화학습 도메인인 스타크래프트2라는 게임의 SC2LE(스타크래프트2 학습환경)을 소개한다.
스타크래프트는 실시간 전략 시뮬레이션(RTS) 게임인데, 빠른 micro-action과 높은 수준의 전략수립과 실행능력을 요구한다. 
지난 20년 동안, 스타크래프트1과 2는 e-sports의 개척자였으며, 수많은 스타 플레이어를 탄생시켰다.
따라서 최고 수준의 플레이어를 이기는 것은 의미있으며 장기적인 목표가 될 수 있다.
<br />
강화학습 관점으로 스타크래프트2는 기존의 한계를 뛰어넘을 만한 최고의 기회를 제공한다. 
첫째, 스타크래프트2는 여러 명의 플레이어가 자원을 놓고 경쟁하는 멀티에이전트 문제이다.
또한 각 플레이어들은 공통 목표를 달성하기 위해 수백개의 유닛을 컨트롤 하며 협동하는 lower-level 멀티에이전트라고 볼 수 있다. 
둘째, 스타크래프트2는 불완전한 정보 게임이다. 맵은 오직 로컬 카메라를 통해 일부만 관찰가능하며, 
플레이어는 정보를 얻기위해 적극적으로 움직여야 한다. 
더욱이, 지도에서 정찰하지 않은 곳이 흐리게 표현되는 전장안개(fog-of-war)개념은 
적의 상태(state)를 판단하기 위해 플레이어의 적극적인 움직임을 요구한다.
셋째, 활동공간(action space) 는 광활하게 넓으며 그 종류가 다양하다. 
플레이어는 대략 $10^8$개로 조합된 공간 중 하나의 행동을 선택한다. 
그리고 많은 종류의 서로 다른 유닛과 건물 종류가 있는데, 각각은 유니크한 행동으로 이어지며, 여러 종류의 빌드오더가 존재한다. 
넷째, 이 게임은 수천개의 프레임과 행동이 지속되고 어떤 유닛을 생산할 것인지 빠른 판단(early decision)이 요구되며,
적을 발견하기 전까진 적을 볼 수 없다. 이것은 시간제약적 신뢰할당(temporal credit assignment)과 탐색(exploration)으로 이어진다.
<br />
본 논문은 스타크래프트 강화학습을 수월하게 해주는 인터페이스를 소개한다. 
* 피여의 낮은 해상도 그리드를 이용해 관측과 행동이 정의 된다. 
* 보상(reward)은 스타크래프트2 엔진을 기반으로 컴퓨터 적을 통해 정해진다.
* 간소화된 미니게임 또한 전체 게임맵으로 제공되고 이는 스타크래프트2 전체 게임의 인터페이스로 확장가능하며 관측과 행동이 RGB 픽셀로 드러난다.
* 에이전트는 멀티플레이어 게임에서 마지막 승리 혹은 패배로 순위가 결정된다
* 평가는 실제 사람과 경쟁하기 위해 전체 게임맵으로 제한 된다. 

게다가 우리는 실제 사람의 리플레이에 기반한 대량의 데이터셋을 제공하는데 이는 실제 사람이 수백만번 플레이한 것과 같다. 
우리는 인터페이스 조합과 이 데이터셋이 기존에 존재하거나 새로운 강화학습 알고리즘을 테스트하기 위한 유용한 벤치마크를 제공할 것이라 믿을뿐만아니라,
퍼셉트론, 메모리와 attention, 순차적 예측(sequence prediction), 불확실성 모델링 등 관점에서 흥미로운 측면이 존재하는데 
이는 모두 머신러닝 연구가 활발한 분야이다.
<br />
몇몇 환경에서 이미 스타크래프트1 강화학습이 존재한다.
우리 연구는 이전의 환경들과는 다소 다른 측면이 있다.
* 우리는 스타크래프트1이 아닌 스타크래프트2를 이용한다.
* 관측과 행동은 프로그램적이라기 보단, 실제 인간 유저 인터페이스에 기반한다.
* 해당 게임개발사인 블리자드 엔터테인먼트의 서포트를 받는다.
이전 연구 환경에 기반한 현존 최강 스타크래프트 인공지능이라 할지라도 실제 인간 아마추어조차 이길 수 없었다. 
이런면에서 보면, 스타크래프트는 흥미로운 게임 플레이 특성과 넓은 유저 기반이 심층 강화학습 연구를 가능하게 한다. 
<br />

## 2. 관련 연구
