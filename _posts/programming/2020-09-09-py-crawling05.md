---
title: "[python] 파이썬 웹 크롤링(5): 데이터 저장하기" 
categories:
  - programming
tags:
  - programming
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# 파이썬 웹 크롤링(5): 데이터 저장하기 

본 포스팅은 Web Scraping with Python by Ryan Mitchell을 참고하였습니다. 

**참고 링크**

* [파이썬 웹 크롤링(1): 크롤링의 기본구조와 BeautifulSoup 설치](https://losskatsu.github.io/programming/py-crawling01/)
* [파이썬 웹 크롤링(2): 데이터 파싱하기](https://losskatsu.github.io/programming/py-crawling02/)
* [파이썬 웹 크롤링(3): 본격적인 크롤링](https://losskatsu.github.io/programming/py-crawling03/)
* [파이썬 웹 크롤링(4): API 활용하기](https://losskatsu.github.io/programming/py-crawling04/) 

# Chapter 5: 데이터 저장하기 
 
## 5.1 데이터 저장의 필요성

데이터를 크롤링해서 바로 print로 확인하는 것은 재밌지만 수많은 데이터를 취합하고 분석하기에는 적절하지 않습니다. 
여러분이 직접 개입하지 않고 웹 크롤링을 유용하게 하려면 데이터를 따로 저장할 필요가 있습니다. 
이번 챕터에서는 데이터 관리에 대해 알아보겠습니다. 
여러분은 혹시 백엔드나 여러분이 스스로 만든 API가 필요하시나요? 
아마 크롤러가 여러분의 데이터베이스에 바로 쓰는(write)하는 것을 원하실지도 모르겠습니다. 
인터넷이 연결되어 있지 않은 경우에도 데이터를 보고 싶다면 파일 스트림을 만들어야겠지요. 
그리고 경고창이나 하루에 한번 취합 정보를 받아보고 싶다면 여러분 이메일주소로 이메일을 보내게끔 하면 됩니다. 


## 5.2 미디어, 이미지 파일 저장하기

미디어 파일(media file)을 저장하는 방법에는 두가지 방법이 있습니다. 
참조하거나 다운로드 받는 것입니다. 
참조하는 방법은 그저 미디어의 URL을 저장하면 됩니다. 
참조 방식의 장점은 아래와 같은 것들이 있습니다. 

* 크롤러가 빠르게 작동합니다. 파일 전체를 다운로드 받을 필요가 없기 때문입니다.
* 하드디스크 용량을 아낄 수 있습니다. 
* 호스트 서버에 부하를 주지 않습니다.

반대로 참조방식의 단점은 아래와 같습니다. 

* 여러분의 웹사이트나 애플리케이션에 URL 링크를 걸어놨다면 여러분의 웹사이트나 애플리케이션이 해당 URL을 위한 하나의 경로가 됩니다. 
* 여러분의 애플리케이션이 URL 호스트 서버와 계속 통신하는 서버 사이클에 속하게 됩니다. 
* 참조 URL은 바뀔 수 있습니다. 이 경우 해당 URL은 더이상 사용할 수 없고 변경을 해줘야합니다. 

파이썬 3에서는 아래 코드로 URL로부터 파일을 다운로드할 수 있습니다. 

```python
urllib.request.urlretrieve
```

```python
from urllib.request import urlretrieve
from urllib.request import urlopen
from bs4 import BeautifulSoup

html = urlopen("http://www.pythonscraping.com")
bsObj = BeautifulSoup(html)
imageLocation = bsObj.find("a", {"id": "logo"}).find("img")["src"]
urlretrieve (imageLocation, "logo.jpg")
```

위 코드는 http://www.pythonscraping.com 라는 주소에서 logo.jpg파일을 저장하라는 뜻입니다. 
이때 저장 위치는 위 코드가 돌아가는 위치입니다. 
이런 방법은 단 하나의 파일을 다운로드할 때 유용합니다. 
하지만 대부분의 크롤러는 단 하나의 파일반 받는 경우는 거의 없습니다. 
아래 코드는 사이트에 존재하는 모든 내부 파일을 다운로드 합니다. 
이때 어느 종류의 태그인지에 상관없이 src 속성에 해당하는 파일을 받습니다. 

```python
import os
from urllib.request import urlretrieve
from urllib.request import urlopen
from bs4 import BeautifulSoup

downloadDirectory = "downloaded"
baseUrl = "http://pythonscraping.com"

def getAbsoluteURL(baseUrl, source):
  if source.startswith("http://www."):
    url = "http://"+source[11:]
  elif source.startswith("http://"):
    url = source
  elif source.startswith("www."):
    url = source[4:]
    url = "http://"+source
  else:
    url = baseUrl+"/"+source
  if baseUrl not in url:
    return None
  return url

def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):
  path = absoluteUrl.replace("www.", "")
  path = path.replace(baseUrl, "")
  path = downloadDirectory+path
  directory = os.path.dirname(path)

  if not os.path.exists(directory):
    os.makedirs(directory)

  return path

html = urlopen("http://www.pythonscraping.com")
bsObj = BeautifulSoup(html)
downloadList = bsObj.findAll(src=True)

for download in downloadList:
  fileUrl = getAbsoluteURL(baseUrl, download["src"])
  if fileUrl is not None:
    print(fileUrl)

urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))
```

## 5.3 CSV 파일 형태로 저장하기



