---
title: "[머신러닝] 릿지(ridge), 라쏘(lasso) 회귀분석 개념" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# 릿지(ridge), 라쏘(lasso) 회귀분석 개념

참고링크

* [라그랑주 듀얼 함수](https://losskatsu.github.io/mathematics/dual-function/)
* [Karush-Kuhn-Tucker](https://losskatsu.github.io/mathematics/kkt/)
* [단순 회귀분석](https://losskatsu.github.io/statistics/simple-regression/)
* [로지스틱 회귀분석](https://losskatsu.github.io/statistics/logistic-regression/)
* [릿지(ridge), 라쏘(lasso) 회귀분석](https://losskatsu.github.io/machine-learning/l1l2/)

## 회귀분석 기초 복습

우리가 추정하고자 하는 모델을 아래와 같다고 하겠습니다. 

$$ y = f(z)+\epsilon, \, \, \, \epsilon ~(0, \sigma^2) $$

그리고 우리가 만든 추정회귀함수를 아래와 같다고 하겠습니다. 

$$ \hat{f(x)} = z^T\hat{\beta} $$

또한 최소제곱법(least squared)으로 구한 최소제곱 추정 모수를 $\hat{\beta}^{ls}$라고 하겠습니다. 
이 때, 우리가 살펴봐야하는 점은 

1. $\hat{\beta}$이 실제 $\beta$에 충분히 가까운가?

2. $\hat{f(x)}$이 미래 관측값에 대해 잘 피팅(fit) 되는가?

1번에 대해 우리는 아래와 같이 MSE(mean squared error)를 사용합니다. 

$$ MSE(\hat{\beta}) = E[\Vert \hat{\beta}-\beta \Vert^2] = E[(\hat{\beta}-\beta)^T(\hat{\beta}-\beta)] $$

최소제곱법에 의하면, 

$$ E[(\hat{\beta}^{ls}-\beta)^T(\hat{\beta}^{ls}-\beta)] = \sigma^{2}tr[(Z^{T}Z)^{-1}] $$

## 릿지(ridge) 회귀분석

제약이 없으면, $\beta_j$가 폭발(explode)적으로 커질수 있고, 이는 분산이 커지는 문제를 초래합니다. 
따라서 이러한 문제를 막기위해 제약식을 사용하는데, 릿지 회귀분석(ridge regression)은 아래와 같은 제약식을 사용합니다. 

$$ minimize \sum_{i=1}^{n}()^2 \,   \, \, \, s.t. \sum_{j=1}^{p}\beta_{j}^{2} \leq t $$ 

이는 아래와 같이 표현할 수도 있습니다. 

$$ minimize (y-Z\beta)^T(y-Z\beta),  \, \, \, s.t. \sum_{j=1}^{p}\beta_{j}^{2} \leq t $$

<center><img src="/assets/images/ml/l1l2/01.jpg" width="800"></center>


<center><img src="/assets/images/ml/l1l2/02.jpg" width="800"></center>

<center><img src="/assets/images/ml/l1l2/03.jpg" width="800"></center>



## 라쏘(lasso) 회귀분석

<center><img src="/assets/images/ml/l1l2/04.jpg" width="800"></center>
