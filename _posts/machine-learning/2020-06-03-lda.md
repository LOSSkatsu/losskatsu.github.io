---
title: "[머신러닝] LDA(Linear Discriminant Analysis)의 개념" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# LDA(Linear Discriminant Analysis)의 개념

**참고링크**
* [ROC 커브 복습하기](https://losskatsu.github.io/machine-learning/stat-roc-curve/)
* [교차검증(cross validataion)](https://losskatsu.github.io/machine-learning/cross-validation/)
* [k-means클러스터링 복습하기](https://losskatsu.github.io/machine-learning/kmeans-clustering/)
* [k-최근접 이웃 알고리즘 복습하기](https://losskatsu.github.io/machine-learning/knn/)
* [선형회귀분석 복습하기](https://losskatsu.github.io/statistics/simple-regression/)
* [로지스틱 회귀분석 복습하기](https://losskatsu.github.io/statistics/logistic-regression/)
* [릿지, 라쏘 회귀분석 북습하기](https://losskatsu.github.io/machine-learning/l1l2/)
* [의사결정나무 복습하기](https://losskatsu.github.io/machine-learning/decision-tree/)
* [서포트벡터머신 복습하기](https://losskatsu.github.io/machine-learning/svm/)
* [LDA 복습하기](https://losskatsu.github.io/machine-learning/lda/)
* [가우시안 혼합 모형(GMM) 복습하기](https://losskatsu.github.io/machine-learning/gmm/)
* [딥러닝 기초 복습하기](https://losskatsu.github.io/machine-learning/dl-basic01/)
* [부스팅(boosting) 복습하기](https://losskatsu.github.io/machine-learning/boosting/)
* [사이킷런 실습하기](https://losskatsu.github.io/machine-learning/sklearn/)

## LDA(Linear Discriminant Analysis)의 정의


$P(G\|X)$ : 클래스 사후확률(posterios) 

$ f_k(x) $: 클래스 G=k일 때의 확률변수 X의 조건 확률 밀도 함수(class density) 

$ \pi_k $: 클래스 k의 사전확률(prior probability) 

위 식을 이용해 베이즈 정리를 이용해 클래스 사후확률 $P(G\|X)$을 구하면 다음과 같습니다. 

$$
\begin{align}
P(G=k|X=x) &= \frac{P(G=k, X=x)}{P(X=x)} \\
           &= \frac{P(X=x|G=k)P(X=x)}{P(X=x)} \\
           &= \frac{f_k(x)\pi_k}{\sum_{l=1}^{K}f_l(x)\pi_l} \\
\end{align}
$$

클래스 확률 밀도 함수 $f_k(x)$가 아래와 같은 다변량 정규분포(Multivariate normal distribution)를 따른다고 합시다. 

$$ f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k)} $$

LDA에서는 모든 클래스 별 확률밀도함수가 모두 같은 공분산을 가지고 있다고 가정합니다. 

$$ \Sigma_k = \Sigma $$ 

서로다른 클래스 $k, l$를 비교하기위해 log-ratio를 보겠습니다. 

$$
\begin{align}
log\frac{P(G=k|X=x)}{P(G=l|X=x)} &= log\frac{f_k(x)}{f_l(x)} + log\frac{\pi_k}{\pi_l} \\
                  &= log\frac{\pi_k}{\pi_l} -\frac{1}{2}(\mu_k + \mu_l)^{T}\Sigma^{-1}(\mu_k-\mu_l) + x^{T}\Sigma^{-1}(\mu_k-\mu_l)
\end{align}
$$

위 식에서 최종 형태를 보시면 $x$에 대한 **선형방정식**이라는 것을 알 수 있습니다. 
저희는 위에서 클래스별 공분산 행렬이 같다고 가정했으므로 약분되는부분이 많아 계산하기는 좀더 쉬워졌네요. 
위의 로그-오즈(log-odds) 함수가 의미하는 것은 클래스 $k$ 집단과 클래스 $l$ 집단의 결정 경계(decision boundary)를 알 수 있다는 것인데요. 
즉, $P(G=k|X=x)=P(G=l|X=x)$인 부분은 $x$에 대해 선형이며 
$p$차원의 [초평면(hyperplane)](https://losskatsu.github.io/mathematics/convex-set/)이라는 것을 알 수 있습니다. 
따라서 모든 결정경계는 선형(linear)입니다. 
만약 우리가 전체 공간 $\mathbb{R}^p$를 클래스별 영역으로 나눈다면, 각 클래스 영역은 초평면에의해 분할 됩니다. 
이 때 중요한 것은 결정경계는 중심에 연결된 선분의 수직 이등분선이 아니라는 것입니다. 
결정경계가 중심에 연결된 선분의 수직이등분선이 되려면 공분산 행렬이 $\Simga = \sigma^2 \mathsf{I}$이어야 하고, 
클래스 사전분포 $\pi_k$가 같아야 합니다. 

다시 한번 클래스 $k$에 대한 선형판별함수(linear discriminant function)을 봅시다.

$$ \delta_k(x) = x^{T}\Sigma^{-1}\mu_k - \frac{1}{2}\mu_{k}^{T}\Simga^{-1}\mu_k + log\pi_k $$ 

즉, 데이터 $x$가 어떤 클래스에 속하는 지는 $G(x) = argmax_{k}\delta_k(x)$를 구하면 알 수 있습니다. 

위에서 각 데이터는 다변량 정규분포를 따른다고 했습니다. 
하지만 저희는 아직 [모수](https://losskatsu.github.io/statistics/population-sample/)를 모르기떄문에 아래와 같이 모수 추정을 하겠습니다. 

$$ \hat{\pi}_{k} = \frac{N_k}{N} $$

$$ \hat{\mu}_k = \sum_{g_i = k}\frac{x_i}{N_k} $$

$$ \hat{\Sigma} = \sum_{k=1}^{K}\sum_{g_i = k}\frac{(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^{T}}{(N - K)}$$ 

