---
title: "[머신러닝] LDA(Linear Discriminant Analysis)의 개념" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# LDA(Linear Discriminant Analysis), QDA(Quadratic discriminant Analysis)의 개념

**참고링크**
* [ROC 커브 복습하기](https://losskatsu.github.io/machine-learning/stat-roc-curve/)
* [교차검증(cross validataion)](https://losskatsu.github.io/machine-learning/cross-validation/)
* [k-means클러스터링 복습하기](https://losskatsu.github.io/machine-learning/kmeans-clustering/)
* [k-최근접 이웃 알고리즘 복습하기](https://losskatsu.github.io/machine-learning/knn/)
* [선형회귀분석 복습하기](https://losskatsu.github.io/statistics/simple-regression/)
* [로지스틱 회귀분석 복습하기](https://losskatsu.github.io/statistics/logistic-regression/)
* [릿지, 라쏘 회귀분석 북습하기](https://losskatsu.github.io/machine-learning/l1l2/)
* [의사결정나무 복습하기](https://losskatsu.github.io/machine-learning/decision-tree/)
* [서포트벡터머신 복습하기](https://losskatsu.github.io/machine-learning/svm/)
* [LDA 복습하기](https://losskatsu.github.io/machine-learning/lda/)
* [가우시안 혼합 모형(GMM) 복습하기](https://losskatsu.github.io/machine-learning/gmm/)
* [딥러닝 기초 복습하기](https://losskatsu.github.io/machine-learning/dl-basic01/)
* [부스팅(boosting) 복습하기](https://losskatsu.github.io/machine-learning/boosting/)
* [사이킷런 실습하기](https://losskatsu.github.io/machine-learning/sklearn/)

## LDA(Linear Discriminant Analysis)의 개념 


$P(G\|X)$ : 클래스 사후확률(posterios) 

$ f_k(x) $: 클래스 G=k일 때의 확률변수 X의 조건 확률 밀도 함수(class density) 

$ \pi_k $: 클래스 k의 사전확률(prior probability) 

위 식을 이용해 베이즈 정리를 이용해 클래스 사후확률 $P(G\|X)$을 구하면 다음과 같습니다. 

$$
\begin{align}
P(G=k|X=x) &= \frac{P(G=k, X=x)}{P(X=x)} \\
           &= \frac{P(X=x|G=k)P(X=x)}{P(X=x)} \\
           &= \frac{f_k(x)\pi_k}{\sum_{l=1}^{K}f_l(x)\pi_l} \\
\end{align}
$$

클래스 확률 밀도 함수 $f_k(x)$가 아래와 같은 다변량 정규분포(Multivariate normal distribution)를 따른다고 합시다. 

$$ f_k(x) = \frac{1}{(2\pi)^{p/2}|\Sigma_k|^{1/2}}e^{-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k)} $$

LDA에서는 모든 클래스 별 확률밀도함수가 모두 같은 공분산을 가지고 있다고 가정합니다. 

$$ \Sigma_k = \Sigma $$ 

서로다른 클래스 $k, l$를 비교하기위해 log-ratio를 보겠습니다. 

$$
\begin{align}
log\frac{P(G=k|X=x)}{P(G=l|X=x)} &= log\frac{f_k(x)}{f_l(x)} + log\frac{\pi_k}{\pi_l} \\
                  &= log\frac{\pi_k}{\pi_l} -\frac{1}{2}(\mu_k + \mu_l)^{T}\Sigma^{-1}(\mu_k-\mu_l) + x^{T}\Sigma^{-1}(\mu_k-\mu_l)
\end{align}
$$

위 식에서 최종 형태를 보시면 $x$에 대한 **선형방정식**이라는 것을 알 수 있습니다. 
저희는 위에서 클래스별 공분산 행렬이 같다고 가정했으므로 약분되는부분이 많아 계산하기는 좀더 쉬워졌네요. 
위의 로그-오즈(log-odds) 함수가 의미하는 것은 클래스 $k$ 집단과 클래스 $l$ 집단의 결정 경계(decision boundary)를 알 수 있다는 것인데요. 
즉, $P(G=k|X=x)=P(G=l|X=x)$인 부분은 $x$에 대해 선형이며 
$p$차원의 [초평면(hyperplane)](https://losskatsu.github.io/mathematics/convex-set/)이라는 것을 알 수 있습니다. 
따라서 모든 결정경계는 선형(linear)입니다. 
만약 우리가 전체 공간 $\mathbb{R}^p$를 클래스별 영역으로 나눈다면, 각 클래스 영역은 초평면에의해 분할 됩니다. 
이 때 중요한 것은 결정경계는 중심에 연결된 선분의 수직 이등분선이 아니라는 것입니다. 
결정경계가 중심에 연결된 선분의 수직이등분선이 되려면 공분산 행렬이 $\Sigma = \sigma^2 \mathsf{I}$이어야 하고, 
클래스 사전분포 $\pi_k$가 같아야 합니다. 

다시 한번 클래스 $k$에 대한 선형판별함수(linear discriminant function)을 봅시다.

$$ \delta_k(x) = x^{T}\Sigma^{-1}\mu_k - \frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_k + log\pi_k $$ 

즉, 데이터 $x$가 어떤 클래스에 속하는 지는 $G(x) = argmax_{k}\delta_k(x)$를 구하면 알 수 있습니다. 

위에서 각 데이터는 다변량 정규분포를 따른다고 했습니다. 
하지만 저희는 아직 [모수](https://losskatsu.github.io/statistics/population-sample/)를 모르기떄문에 아래와 같이 모수 추정을 하겠습니다. 

$$ \hat{\pi}_{k} = \frac{N_k}{N} $$

$$ \hat{\mu}_k = \sum_{g_i = k}\frac{x_i}{N_k} $$

$$ \hat{\Sigma} = \sum_{k=1}^{K}\sum_{g_i = k}\frac{(x_i - \hat{\mu}_k)(x_i - \hat{\mu}_k)^{T}}{(N - K)}$$ 

예를 들어 두개의 클래스1, 2가 있다고 하고 LDA에 의해 어떤 데이터가 클래스2에 속하려면 아래와 같은 조건을 만족해야합니다. 

$$ x^{T}\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1) - \frac{1}{2}(\hat{\mu}_2 + \hat{\mu}_1)^{T}\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)  + log\frac{N_2}{N_1}> 0 $$

위 식을 조금 바꾸면 아래와 같습니다. 

$$ x^{T}\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1) > \frac{1}{2}(\hat{\mu}_2 + \hat{\mu}_1)^{T}\hat{\Sigma}^{-1}(\hat{\mu}_2 - \hat{\mu}_1)  - log\frac{N_2}{N_1}> 0 $$

위에서 데이터가 다변량 정규분포를 따른다고 가정했지만, 꼭 다변량 정규분포여야지만 LDA를 쓸 수 있는 것은 아닙니다. 


## QDA(Quadratic discriminant Analysis)의 개념

앞선 LDA에서는 클래스 집단 별 공분산 행렬 $\Sigma_k$가 모두 동일하다고 했습니다. 
이걸 조금 더 일반화시켜서 클래스 집단 별 공분산 행렬이 같다는 가정이 없다고 하겠습니다. 
그러면 클래스별 로그-비율에서 약분되는 부분이 없어지고 이차판별함수(quadratic discriminant function)은 아래와 같게 됩니다. 

$$ \delta_{k}(x) = -\frac{1}{2}log\|\Sigma_k\| - \frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) + log(\pi_k) $$

위 판별식에서 볼 수 있듯 클래스 $k$와 클래스 $l$의 결정경계(decision boundary)는 $\delta_{k}(x)=\delta_{l}(x)$이고, 
이차식(quadratic equation)을 따릅니다. 위에서 LDA에서는 판별함수가 선형(linear)이었던 것에 반대로, 
QDA에서는 이차식을 따르네요. 

## LDA 결정 경계는 항상 직선?

LDA의 판별함수는 선형을 따르니까 LDA를 이용하면 항상 직선만 그려질까요? 
그렇지 않습니다. LDA를 이용해서도 곡선의 형태로 판별식을 그릴 수 있는데요. 
방법은 바로 데이터 $x$의 공간을 좀 더 높은 차원으로 선형변환 한 후, 확장된 공간에서 LDA를 하는 거죠. 
무슨 말이냐면 예를들어 데이터 $x$가 이차원이라고 해봅시다. 
그러면 $x = (x_1, x_2)$라고 할 수 있는데, 이 2차원 데이터를 5차원으로 확장시켜봅시다. 예를들어, 
$x^{\*} = (x_1, x_2, x_1 x_2, x_{1}^{2},x_{2}^{2})$로 확장시킨 후 확장된 공간에서 LDA를 하는 거죠. 
그러면 결정경계가 곡선의 형태로 나타납니다. 

## QDA vs LDA 모수 추정 개수

우리가 사용하는 결정경계는 결국 데이터의 확률밀도함수의 모수에 대한 함수이므로, 
우리가 추정해야하는 모수 개수를 구하면 우리가 계산해야할 양을 어느정도 알 수 있습니다. 
LDA의 경우, $(K-1)\times(p+1)$개의 모수를 추정해야합니다. 
QDA의 경우에는 $(K-1)\times(p(p+3)/2 + 1)$개의 모수를 추정해야합니다. 

## LDA, QDA가 성능 좋은 이유

LDA나 QDA가 좋은 성능을 보이는 이유는 정규분포를 가정했을 때의 모수 추정치가 안정적(stable)이기 때문입니다. 
여기서 안정적이라는 말은 bias-variance tradeoff를 의미하는데요, 
LDA나 QDA를 사용하면 다른 방법에 비해 어느정도 bias는 감수해야하는데, 
이유는 bias가 어느정도 있는 대신 분산이 작기 때문입니다. 
