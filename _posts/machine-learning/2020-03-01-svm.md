---
title: "[머신러닝] 서포트 벡터 머신(support vector machine) 개념 정리" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# 서포트 벡터 머신(support vector machine) 개념 정리

참고링크
* [회귀분석 복습하기](https://losskatsu.github.io/statistics/simple-regression/)
* [로지스틱 회귀분석 복습하기](https://losskatsu.github.io/statistics/logistic-regression/)
* [서포트 벡터 머신 복습하기](https://losskatsu.github.io/machine-learning/svm/)


## 개요

데이터 셋을 classification 하는 방법에는 여러가지가 있습니다. 
크게 선형(linear)방법과 비선형(nonlinear) 방법이 있는데요. 
선형방법에 대표적인 방법에는 LDA(Linear Discriminant Analysis), 로지스틱회귀분석 등이 있고, 
비선형방법의 대표적인 방법에는 오늘 다룰 서포트 벡터 머신(support vector machine)이 있겠습니다. 
여기서 비선형이란 경계(boundary)가 비선형이라는 말입니다. 
지금부턴 서포트벡터머신을 간단히 svm 이라고 부르겠습니다.

![figure01](/assets/images/ml/svm/svm01.jpg){: width="500"}


## margin 

margin은 svm에서 핵심적인 개념입니다. 한글로 번역하면 '여백'이라고 해야할까요. 
이 여백이 중요한 이유는 데이터셋을 분리시킬때 집단간 간격이 가능한한 가장 넓어야하기 때문입니다. 
참고로 어떤 데이터 포인트가 경계선에서 가능한 한 멀리있을수록 우리의 확신 정도는 강해집니다. 
만약 어떤 점이 경계선 근처에 있다면 경계선이 바뀔 경우 해당 데이터포인트가 속하는 집단이 달라질 수 있겠죠. 
반면 경계선에서 멀~리 떨어져있다면 경계선이 어떻게 바뀌던 데이터포인트가 속하는 집단이 달라질 가능성은 별로 없습니다. 

![figure02](/assets/images/ml/svm/svm02.jpg){: width="500"}

위 그림을 보시면 + 영역, - 영역, 중심선, 경계선이 있습니다. 
그리고 중심선과 수직인 벡터 $\vec{w}$가 존재하는데요. 
이 벡터 $\vec{w}$가 중요합니다. 
svm의 중심 아이디어는 각 데이터포인트 $(x_1, x_2)$과 $\vec{w}$를 내적시켰을 때, 
즉, $(x_1, x_2)$과 $\vec{w}$을 [내적](https://losskatsu.github.io/linear-algebra/innerproduct/)한 값이 특정 상수 이상이면 +영역, 이하이면 -영역으로 정의할 수 있습니다. 

![figure03](/assets/images/ml/svm/svm03.jpg){: width="500"}

이 때, 저희는 아직 $\vec{w} = (w_1, w_2)$와 $c$ 모두 알지 못합니다. 
그리고 이것이 데이터분류를 위해 앞으로 저희가 구해야하는 값입니다. 

### decision rule

![figure04](/assets/images/ml/svm/svm04.jpg){: width="500"}

decision rule은 데이터의 분류를 위한 것입니다. 
위 그림처럼 $\vec{w}$와 $\vec{x}의 [내적](https://losskatsu.github.io/linear-algebra/innerproduct/)값이 c보다 크면 +영역이고, c면 경계선, c보다 작으면 -영역입니다. 이를 일반화 시킨 것이 $\vec{w}\vec{x}+b \geq 0$인데요. 이제부터 $\vec{w}$와 $b$를 구해봅시다. 

![figure05](/assets/images/ml/svm/svm05.jpg){: width="500"}

잠시 $\vec{w}$에 대해 생각해보면 사실 중심선에 [직교](https://losskatsu.github.io/linear-algebra/orthogonal/)하는 벡터는 굉장히 많습니다. 
왜냐하면 위 그림처럼 벡터의 길이를 바꿀 수 있기 때문인데요. 
그렇기 때문에 우리는 $\vec{w}$와 $b$를 구하기 위해서 추가적인 제약조건이 필요합니다. 

![figure06](/assets/images/ml/svm/svm06.jpg){: width="500"}

위 그림처럼 decision rule이 1보다 큰 영역을 +영역이라고 하고, -1보다 작은 영역을 -영역, 그리고 0인 곳이 중심선이라고 합시다. 

![figure07](/assets/images/ml/svm/svm07.jpg){: width="500"}

그렇다면 경계선과 중심선 사이의 값들은 위 그림 처럼 나타낼 수 있을 것입니다. 

