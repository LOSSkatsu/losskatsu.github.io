---
title: "[딥러닝] 딥러닝 기초(2) 성능함수란" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# 딥러닝 기초(2) 성능함수 

## 신경망이 하는 일

뉴런의 집합, 즉 신경망이 하는 일은 무엇을까요? 
아래 그림처럼 가운데 뉴런으로 가득찬 두개골이 있다고 합시다. 
수많은 자극들이 두개골로가면 임계치와 가중치를 이용해 결과값을 출력합니다. 

<center><img src="/assets/images/ml/dl/basic_dl/deepbasic03.jpg" width="800"></center>

즉 신경망을 학습시킬 때 우리가 할 수 있는 것은 
가중치와 임계치를 조절해 우리가 원하는 값을 구하는 것입니다. 
즉, 신경망은 근사값의 함수라고 할 수 있습니다. 

$$ \vec{z} = f(\vec{x}, \vec{w}, \vec{T}) $$

위의 식이 신경망의 결과값 벡터 $\{vec}$에 대한 함수인데요. 
함수 $f$를 보시면 $x$벡터는 인풋이니 주어진 값이고, 우리가 조절할 수 있는 부분은 
가중치 벡터 $w$와 임계값 $T$입니다. 

<center><img src="/assets/images/ml/dl/basic_dl/deepbasic04.jpg" width="800"></center>

위 식을 보시면 우리가 얼마나 잘 하고 있는지를 알 수 있습니다. 
바로 목표값 벡터(desire vector)와 신경망의 결과값을 비교함으로써 가능합니다. 
비교하는 방법은 **성능함수**를 만들어 측정할 수 있는데요. 성능함수는 목표값과 결과값에의해 결정됩니다. 

## 성능함수

성능함수를 만드는 방법은 여러가지가 있는데요. 

<center><img src="/assets/images/ml/dl/basic_dl/deepbasic05.jpg" width="800"></center>

위 그림에서 가장 먼저 생각할 수 있는 케이스는 (a)입니다. 바로 목표값과 결과값의 차이의 [norm](https://losskatsu.github.io/linear-algebra/innerproduct/)인데요. 물론 이 성능함수는 0에 가까울 수록 좋습니다. 0에 가깝다는건 목표값과 결과값의 차이가 거의 없다는 뜻이니까요. 
하지만 (a)식에는 단점이 있습니다. 바로 0에서 미분이 되지 않는 것인데요. 이는 수학적 계산을 불편하게 합니다. 
그래서 나온 방법이 (b)입니다. (a)에 제곱을 한 꼴인데요. 그러면 그래프가 스무스해집니다. 
(b)처럼 되면 0에서도 미분이 가능합니다. 
하지만 (b)에서는 함수의 최소값이 성능의 최대가 됩니다. 
음...이왕이면 함수의 최대값이 성능의 최대가 되는게 더 이해하기 쉽지 않을까요?
그렇게해서 나온것이 (c)입니다. 

## 언덕 오르기 문제

다시 한 번 강조하면, 우리의 목적은 가중치와 임계치를 조절해서 성능을 최대화 하는 것입니다. 
따라서 아래 그림처럼 등고선 지도가 있다고 합시다. 이 지도는 성능에 대한 $w_1, w_2$ 그래프가 인데요. 
초기값 $x$에 대해 성능을 최대화시키는, 즉, 등고선의 중앙으로 이동시켜야합니다. 
이는 언덕오르기 문제라고 볼 수 있습니다. 참고로 이방법은 연속적인 공간에서만 성립 가능합니다. 

<center><img src="/assets/images/ml/dl/basic_dl/deepbasic06.jpg" width="800"></center>

$x$근처의 화살표는 이동가능한 방향인데요. 우리가 움직이고 싶은건 분홍색 화살표 방향이겠죠. 
모든 방향에 대해 전부 계산하는건 오래 걸리니 편미분을 이용하겠습니다. 
즉, $\partial{\mathcal{P}}/\partial{w_1}$은 $w_1$방향으로 조금 움직였을 때 성능에 얼마나 효과가 있는지 알려줍니다. 
마찬가지로 $\partial{\mathcal{P}}/\partial{w_2}는 $w_2$방향으로 조금 움직였을 때 성능에 얼마나 효과가 있는지 알려줍니다. 
만약 $\partial{\mathcal{P}}/\partial{w_2} > \partial{\mathcal{P}}/\partial{w_1}$라면 오른쪽($w_1$)방향으로 움직여야겠지요. 
왜냐하면 성능함수는 작을수록 좋기 때문입니다. 

<center><img src="/assets/images/ml/dl/basic_dl/deepbasic07.jpg" width="800"></center>

이를 하나의 식으로 나타내면 위의 식과 같습니다. 
위 식에서 저희는 속도상수 r을 조정함으로써 성능을 향상시킬 수 있습니다. 
