---
title: "[머신러닝] 가우시안 혼합 모델(Gaussian mixture model) 기초 개념" 
categories:
  - machine-learning
tags:
  - machine-learning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
sidebar:
  title: "AI Machine Learning"
  nav: sidebar-contents
---

# 가우시안 혼합 모델(Gaussian mixture model) 기초 개념

## 가우시안 혼합 모델 정의

> 혼합 모델은 통계학에서 전체 집단안의 하위 집단의 존재를 나타내기 위한 확률 모델이다. 

즉, 가우시안 혼합 모델은 전체 집단의 하위 집단의 [확률분포](https://losskatsu.github.io/statistics/prob-distribution/)가 [가우시안 분포](https://losskatsu.github.io/statistics/normaldist/)를 따르는 경우를 말합니다. 
흔히 정규분포를 가우시안 분포라고도 부르니 혼동 없으시길 바랍니다. 
또한 가우시안 혼합 모델은 비지도학습의 한 종류로, 데이터 클러스터링(clustering)에 사용합니다. 

## 가우시안 혼합 모델 예시

## 우리가 추정해야할 모수

위 그림처럼 전체 집단의 하위 집단이 가우시안 분포 3개라고 가정합니다. 
이 경우, 저희가 추정해야 할 [모수](https://losskatsu.github.io/statistics/population-sample/)는 총 9개 입니다. 하나하나 살펴보죠. 
먼저 분포가 가우시안 분포를 따르는 하위집단 1개만 살펴봅시다. 
가우시안 분포의 경우 모수가 평균($\mu)$, 분산($\sigma$)입니다. 
여기서 하나를 더 추정해야하는데요, 그것은 전체분포에 대한 해당 하위분포의 비율입니다. 
이를 size($\pi$$라고 하는데요, 다른 말로하면 해당 데이터가 해당 하위집단에 속할 확률입니다. 
그럼 정리해서 첫번째 하위집단의 [모수](https://losskatsu.github.io/statistics/population-sample/)는 $(\mu_1, \sigma_1, \pi_1)$ 입니다. 
그럼 자연스럽게 하위집단이 3개이므로, 나머지 두집단의 모수는 $(\mu_2, \sigma_2, \pi_2)$, $(\mu_3, \sigma_3, \pi_3)$가 됩니다. 


## 확률변수 X의 확률밀도함수

그렇다면 위 정보를 토대로 확률변수 X의 확률 밀도 함수를 살펴봅시다. 

$$ p(x) = \sum_{c}\pi_c N(x; \mu_c, \sigma_c)$$

위 식에서 $c$는 $c$번째 집단을 의미합니다. $c=1,2,3$. 
또한 N은 가우시안 분포라는 뜻이구요, $N(x; \mu_c, \sigma_c)$는 평균이 $\mu_c$, 분산이 $\sigma_c$인 가우시안 분포에서 추출한 데이터라는 뜻입니다. 이를 좀더 풀어쓰면 아래와 같은데요. 아래 식은 참고만 하시고 꼭 외울 필요는 없습니다. 


## 참고: 다변량 가우시안 분포(Multivariate Gaussian models)

$$ N(x; \mu, \sum) = \frac{1}{(2\pi)^{d/2}}\|sum\|^{-1/2}exp{-\frac{1}{2}(x-\mu)^{T}\sum^{-1}(x-\mu)}$$

그리고 위 분포의 MLE(Maximum Likelihood Estimates)는 다음과 같습니다. 

$$ \hat{\mu} = \frac{1}{m}\sum_{i}x_i$$

$$ \hat{\sum} = \frac{1}{m}\sum(x_i - \hat{\mu})^{T}(x_i - \hat{\mu})$$

## 잠재변수(latent variable) z 등장

사실 우리가 고려해야하는 [확률변수](https://losskatsu.github.io/statistics/random-variable/)는 하나 더 있습니다. 
이것은 데이터가 집단 c에 속하는 경우, 즉 $z$인데요. 
수식으로는 아래와 같이 표현합니다.  

$$ p(z=c)=\pi_c $$

위 식은 z가 c일 확률이 $\pi_c$라는 뜻입니다. 
이때, [확률변수](https://losskatsu.github.io/statistics/random-variable/) z를 관찰할수 없다고 하여(unobservable) 잠재변수(latent variable)이라고 합니다. 
잠재변수에 대해 한가지 더 알수있는 조건부확률이 있습니다. 이는 다음과 같은데요. 

$$ p(x|z=c) = N(x; \mu_c, \sigma_c)$$

위 식의 의미는 간단합니다. 하위집단이 c일때 확률변수 x는 평균이 $\mu_c$, 분산이 $\sigma_c$인 가우시안 분포를 따른다는 것입니다. 
정리하면, 우리가 알고있는 식은 두가지입니다.

## EM 알고리즘(Expection Maximization Algorithm)

이제 모수를 추정할 것인데요, 전체 과정은 아래와 같이 E-step, M-step 두가지로 구성되어있습니다. 
이를 EM 알고리즘이라고 하는데요. 
아래와 같이 E-step과 M-step을 반복하여 로그가능도함수를 증가시키는 방법을 EM알고리즘이라고 합니다. 
그럼 한단계씩 알아봅시다. 

## E-Step

먼저 $\r_{ic}$를 구합니다. 어떻게 구하냐면요. 

$$ p(z=c)=\pi_c $$

$$ p(x|z=c) = N(x; \mu_c, \sigma_c)$$

위 두가지 조건과 베이즈정리를 이용하면 i번째 데이터가 그룹 c에 속할 확률($\r_{ic}$라고 표현)을 구할 수 있습니다. 

$\r_{ic} = p(z=c|x) = $ 

## M-Step






