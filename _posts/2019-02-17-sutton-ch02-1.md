---
title: "Sutton Ch.02 Multi-armed Bandits" 
categories:
  - ReinforcementLearning
tags:
  - ReinforcementLearning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
---


## Chapter2. Multi-armed Bandits

강화학습이 다른 머신러닝과 차별되는 점은 트레이닝 셋을 올바른 행동(action)을 주기 위함이 아니라, 
행동을 평가(evaluate)하는데 사용한다는 점이다. 
이는 좋은 행동을을 명백히 찾을 수 있는 활발한 exploration 니즈를 만들게 된다. 
단지 그 행동이 얼마나 좋은지, 최악의 행동인지 최선의 행동인지만을 평가하는 것이 아니다. 
강화학습에서의 피드백은 앞으로 어떤 행동을 취해야할지를 나타내는데, 이 때, 실제 어떤 행동을 취하는 지와는 독립적이다. 
<br />

이번 챕터에서는 한 가지 상황에서 간소화된 환경에서의 강화학습의 평가적인 측면을 연구해보자. 

### 2.1 A k-armed Bandit Problem

다음과 같은 학습 상황을 생각해보자. 
당신은 $k$개의 행동 중 하나를 선택하는 상황이 반복된다고 생각해보자. 
당신은 어떤 행동을 선택하느냐에 따라 각 선택으로부터 수치적으로 표현할 수 있는 stationary 확률 분포를 따르는 보상을 받게 된다. 
당신의 목표는 전체 보상의 기대값을 최대화 시키는 것이다. 
예를 들어 1000가지 행동 혹은 time step에 관해서 말이다. 


