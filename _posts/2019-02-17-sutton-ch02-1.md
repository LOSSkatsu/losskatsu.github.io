---
title: "Sutton Ch.02 Multi-armed Bandits" 
categories:
  - ReinforcementLearning
tags:
  - ReinforcementLearning
use_math: true
toc: true
toc_label: "My Table of Contents"
toc_icon: "cog"
---


## Chapter2. Multi-armed Bandits

강화학습이 다른 머신러닝과 차별되는 점은 트레이닝 셋을 올바른 행동(action)을 주기 위함이 아니라, 
행동을 평가(evaluate)하는데 사용한다는 점이다. 
이는 좋은 행동을을 명백히 찾을 수 있는 활발한 exploration 니즈를 만들게 된다. 
단지 그 행동이 얼마나 좋은지, 최악의 행동인지 최선의 행동인지만을 평가하는 것이 아니다. 
강화학습에서의 피드백은 앞으로 어떤 행동을 취해야할지를 나타내는데, 이 때, 실제 어떤 행동을 취하는 지와는 독립적이다. 
<br />

이번 챕터에서는 한 가지 상황에서 간소화된 환경에서의 강화학습의 평가적인 측면을 연구해보자. 

### 2.1 A k-armed Bandit Problem

다음과 같은 학습 상황을 생각해보자. 
당신은 $k$개의 행동 중 하나를 선택하는 상황이 반복된다고 생각해보자. 
당신은 어떤 행동을 선택하느냐에 따라 각 선택으로부터 수치적으로 표현할 수 있는 stationary 확률 분포를 따르는 보상을 받게 된다. 
당신의 목표는 전체 보상의 기대값을 최대화 시키는 것이다. 
예를 들어 1000가지 행동 혹은 time step에 관해서 말이다. 
<br />

k-armed bandit problem의 예
* 슬롯 머신(one-armed bandit problem), 레버를 한번 당길때마다 주어지는 보상문제.
* 의사의 수술 문제, 의사가 어떤 환자를 치료하는데 k가지 치료범 중 하나를 선택. 

<br />
이것이 k-armed bandit problem의 원래 형태이다. 
우리가 다루는 k-armed bandit problem은 

* $value$: $k$개의 행동을 선택했을 때의 평균(기대값) 보상
* $A_t$: time step $t$에 대해 선택한 행동(action)
* $R_t$: time step $t$에 대해 선택한 행동(action)에 대한 보상 
* $q_{*}(a)$: 임의의 행동 $a$를 선택했을 때의 $value$

$$ q_{*} \doteq E[R_t|A_t = a] $$

만약 당신이 각 행동에 대한 value를 안다면 k-armed bandit problem 을 푸는게 쉬워질 것이다. 
왜냐면 당신은 가장 높은 value를 가지는 행동만 선택하면 되기 때문이다.  
우리는 당신이 그저 추정할 수 있을 뿐이지 action value를 모른다고 가정할 것이다. 

* $Q_t(a)$: time step $t$에서 행동 $a$에 대한 추정 value 
* $Q_t(a)$는 $q_{*}(a)$ 에 수렴할 것이다.




